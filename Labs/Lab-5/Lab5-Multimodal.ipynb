{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac06074d-b80f-4cc9-a54b-43fdca5e31ca",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"../mlu_utils/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">Multimodal Generative AI</a>\n",
    "## <a name=\"0\">Lab 5: Multimodal Inference</a>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "    \n",
    "This exercise shows how multimodal embeddings can be used for search and retrieval with a sample product dataset. Amazon Titan Multimodal embeddings and the Amazon OpenSearch service will be utilized to enable semantic search based on multiple modalities like title, description and images. This allows finding related products that share conceptual and visual similarities beyond just textual matches.\n",
    "    \n",
    "1. <a href=\"#1\">Installing_dependencies</a>\n",
    "2. <a href=\"#2\">Convert Image to base64</a>\n",
    "3. <a href=\"#3\">Make Multimodal Predictions</a>\n",
    "4. <a href=\"#4\">Prompting With Images</a>\n",
    "5. <a href=\"#5\">Best Practices for Multimodal Prompting</a>\n",
    "6. <a href=\"#6\">Use Cases</a>\n",
    "    1. <a href=\"#6.1\">Explain Images</a>\n",
    "    2. <a href=\"#6.2\">Information Retrieval</a>\n",
    "    3. <a href=\"#6.3\">Text Extraction and Transcription</a>\n",
    "    4. <a href=\"#6.4\">Analyze Charts and Graphs</a>\n",
    "    5. <a href=\"#6.5\">Improve Accessibility</a>\n",
    "    6. <a href=\"#6.6\">Complex Reasoning and Analysis of Documents</a>\n",
    "7. <a href=\"#7\">Quizzes</a>\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb1e850-40f9-4f6f-87a8-95cd0beb3184",
   "metadata": {},
   "source": [
    "### <a name=\"1\">Installing dependencies</a>\n",
    "**Note:** the pip command below might output some error messages. You can disregard them, as they are not affecting the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748de71e-4b81-41b4-921a-c8bb86c1ea62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca14cd31-99dd-42f1-9d7a-9dee68f74872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import httpx\n",
    "from IPython.display import Image, display, Markdown, IFrame\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47839611-37bc-4350-bd07-1db41ba6e9b3",
   "metadata": {},
   "source": [
    "### <a name=\"2\">Convert Image to base64</a>\n",
    "\n",
    "There are scenarios where you need to transmit or store image data along with other types of data, such as text or structured data. However, images are inherently binary data, and directly embedding them in text-based formats like JSON or HTML can be problematic. This is where Base64 encoding comes into play, providing a way to represent binary data as text.\n",
    "\n",
    "[Base64](https://en.wikipedia.org/wiki/Base64) is an encoding scheme that converts binary data into a sequence of printable ASCII characters. This encoding is particularly useful when you need to embed binary data (like images) in text-based formats or protocols, such as JSON, HTML, or HTTP requests and responses. This can be particularly useful when working with multimodal models like Claude3, which can process both text and images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ba4f3-1693-4554-a311-507935199425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_base64_encoded_image(image_paths):\n",
    "    \"\"\"\n",
    "    Encode one or more image files or URLs into base64 strings.\n",
    "\n",
    "    Args:\n",
    "        image_paths (str or list): A single file path/URL or a list of file paths/URLs.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two lists:\n",
    "            - A list of base64-encoded image strings.\n",
    "            - A list of corresponding image types.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unsupported image format is encountered.\n",
    "    \"\"\"\n",
    "    # Convert input to list if it's a single string\n",
    "    if isinstance(image_paths, str):\n",
    "        image_paths = [image_paths]\n",
    "\n",
    "    images, image_types = [], []\n",
    "\n",
    "    # Iterate over the image paths/URLs\n",
    "    for path in image_paths:\n",
    "        # Check if the path is a URL\n",
    "        if path.startswith(\"https://\"):\n",
    "            # url_content = requests.get(path).content\n",
    "            url_content = httpx.get(path).content\n",
    "            base64_encoded_data = base64.b64encode(url_content)\n",
    "            base64_string = base64_encoded_data.decode('utf-8')\n",
    "        # Otherwise, assume it's a file path\n",
    "        else:\n",
    "            with open(path, \"rb\") as image_file:\n",
    "                binary_data = image_file.read()\n",
    "                base64_encoded_data = base64.b64encode(binary_data)\n",
    "                base64_string = base64_encoded_data.decode('utf-8')\n",
    "\n",
    "        # Determine the image type based on the file extension\n",
    "        if path.endswith('.png'):\n",
    "            image_type = 'image/png'\n",
    "        elif path.endswith('.jpg') or path.endswith('.jpeg'):\n",
    "            image_type = 'image/jpeg'\n",
    "        elif path.endswith('.webp'):\n",
    "            image_type = 'image/webp'\n",
    "        elif path.endswith('.gif'):\n",
    "            image_type = 'image/gif'\n",
    "        else:\n",
    "            raise ValueError(\"Only 'jpeg', 'png', 'webp', and 'gif' image formats are currently supported\")\n",
    "\n",
    "        images.append(base64_string)\n",
    "        image_types.append(image_type)\n",
    "\n",
    "    return images, image_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dfd849-2d1a-4767-bb51-e0b141527f16",
   "metadata": {},
   "source": [
    "### <a name=\"3\">Make Multimodal Predictions</a>\n",
    "\n",
    "Multimodal prompting is a technique that allows language models to process and generate responses based on a combination of text and image inputs. The input consists of textual prompts or questions along with one or more images, often encoded in base64 format with types specified. The language model's architecture and training allow it to understand the relationships between text and visual information. This enables more contextual and visually grounded interactions, useful for applications like image captioning, visual question answering, and multimodal content generation. The generated response can be textual descriptions, answers, or new multimodal outputs, leveraging both modalities for enhanced capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a9a9d-5282-4353-be00-b3aa5dadf137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_claude_3_multimodal(prompt, images, image_types):\n",
    "    \"\"\"\n",
    "    Invoke the Claude-3 multimodal model from Anthropic using AWS Bedrock runtime.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The text prompt to provide to the model.\n",
    "        images (list): A list of base64-encoded image data.\n",
    "        image_types (list): A list of MIME types corresponding to the images.\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response text.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid model name is provided.\n",
    "    \"\"\"\n",
    "    # Initialize the Amazon Bedrock runtime client\n",
    "    client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "    model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "    # Prepare the multimodal prompt message\n",
    "    message_content = []\n",
    "\n",
    "    # Add each image to the message content\n",
    "    for image, img_type in zip(images, image_types):\n",
    "        message_content.append({\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": img_type,\n",
    "                \"data\": image,\n",
    "            },\n",
    "        })\n",
    "    message_content.append({\"type\": \"text\", \"text\": prompt})\n",
    "\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2048,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1.0,\n",
    "        \"top_k\": 250,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": message_content,\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            body=json.dumps(request_body),\n",
    "        )\n",
    "\n",
    "        # Process and return the response\n",
    "        result = json.loads(response.get(\"body\").read())\n",
    "        return result['content'][0]['text']\n",
    "\n",
    "    except ClientError as err:\n",
    "        logger.error(\n",
    "            \"Couldn't invoke Claude 3 %s model. Here's why: %s: %s\",\n",
    "            model_id.capitalize(),\n",
    "            err.response[\"Error\"][\"Code\"],\n",
    "            err.response[\"Error\"][\"Message\"],\n",
    "        )\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3089c22f-b36b-4f87-8a81-3e095b4b77c0",
   "metadata": {},
   "source": [
    "## <a name=\"4\">Prompting With Images</a>\n",
    "\n",
    "Let's try out the multimodal models using prompts containing both images and text.\n",
    "We will use the following [image](https://stock.adobe.com/search/images?filters%5Bcontent_type%3Aphoto%5D=1&filters%5Bfetch_excluded_assets%5D=1&filters%5Binclude_stock_enterprise%5D=1&filters%5Bcontent_type%3Aimage%5D=1&order=relevance&price%5B%24%5D=1&limit=100&search_type=usertyped&search_page=1&k=amazon+package&acp=&aco=amazon+package&get_facets=1&asset_id=419867610) to explore the visual understanding capabilities of Claude3 Sonnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbdcd1f-7187-41d5-a691-da8be4588a17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# View Image\n",
    "Image(filename='content/Amazon-Packages.jpeg', width=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94523109-b4b5-46a3-9bfa-1d5dd44bd68b",
   "metadata": {},
   "source": [
    "#### Loading images from local path\n",
    "\n",
    "To prompt Claude3 with local images, convert the image files to base64 encoding and pass them as a list in the \"image\" parameter of the multimodal request. This allows Claude3 to analyze the base64-encoded images along with the text prompt.\n",
    "\n",
    "We will use the helper functions `get_base64_encoded_image` and `invoke_claude_3_multimodal` for prompting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d01d3-9704-4c7a-90ed-98ef056c39dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Describe the image.\"\n",
    "image_string, image_type = get_base64_encoded_image(\"content/Amazon-Packages.jpeg\")\n",
    "\n",
    "response = invoke_claude_3_multimodal(prompt=prompt, images=image_string, image_types=image_type)\n",
    "Markdown(\"<i>\"+response+\"</i>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2b22e-239e-4e91-8fee-8aa2affc4228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Where is the largest box located?\"\n",
    "image_string, image_type = get_base64_encoded_image(\"content/Amazon-Packages.jpeg\")\n",
    "\n",
    "response = invoke_claude_3_multimodal(prompt=prompt, images=image_string, image_types=image_type)\n",
    "Markdown(\"<i>\"+response+\"</i>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd4b48-0721-4c09-a0d0-94e7ca1cae75",
   "metadata": {},
   "source": [
    "Multimodal models are very powerful, yet they may make mistakes when analyzing images as we might observe in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0d7b7-dadd-4130-82e6-8ec9ab0d64f7",
   "metadata": {},
   "source": [
    "#### Loading images from url\n",
    "\n",
    "You can also directly load images from url. The process remains pretty similar as the previous example. The image is retrieved from the url, converted into base64 string and then passed to the model.\n",
    "\n",
    "Photo by Pixabay from Pexels under Creative Commons license (CC0): https://images.pexels.com/photos/66709/pexels-photo-66709.jpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d561c-bb79-45f0-b481-c9ce177d595e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_url = \"https://images.pexels.com/photos/66709/pexels-photo-66709.jpeg\"\n",
    "Image(url=image_url, width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10bd75-4715-41f6-b90a-9133e91b035d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Write a sonnet about the object in the image.\"\n",
    "image_string, image_type = get_base64_encoded_image(image_url)\n",
    "\n",
    "response = invoke_claude_3_multimodal(prompt=prompt, images=image_string, image_types=image_type)\n",
    "Markdown(\"<i>\"+response+\"</i>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b926a8-f46e-45b3-b753-071c5bc3d215",
   "metadata": {},
   "source": [
    "## <a name=\"5\">Best Practices for Multimodal Prompting</a>\n",
    "\n",
    "As multimodal models become more advanced and capable of understanding different data modalities like text, images, and audio, it's crucial to follow established best practices when prompting these models. Adhering to guidelines helps ensure the outputs generated are relevant and expected.\n",
    "\n",
    "Maintain standard best practices for prompting such as providing clear and specific prompts, avoiding ambiguity, and carefully curating the data inputs. Best practices also emphasize the importance of maintaining user privacy and respecting intellectual property rights. For instance, being thoughtful about what images or audio clips are uploaded to avoid unintentionally sharing personal or copyrighted content.  \n",
    "\n",
    "Moreover, best practices guide users on how to interpret model outputs appropriately, understanding the inherent limitations of AI systems and that responses should be validated against authoritative sources. Following these recommendations ultimately leads to more trustworthy and reliable interactions with cutting-edge AI capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2caf9-982a-464b-8af1-a5be1656f647",
   "metadata": {},
   "source": [
    "#### Know your model's capabilities and limitations\n",
    "\n",
    "It is important to consider the selected model's known capabilities and limitations while prompting the model. In this lab, we are using the Claude3 models which have the following guidelines for effective prompting:\n",
    "\n",
    "Claude 3 Models:\n",
    "- **Input Format**: Images need to be provided in a base64-encoded format.\n",
    "- **Image Size**: Individual image size cannot exceed 5MB\n",
    "- **Multiple Images**: Claude 3 models supports prompting with up to 5 images.\n",
    "- **Image Format**: Supported image formats: 'PNG', 'JPEG', 'WebP' and 'GIF'.\n",
    "- **Image Clarity**: Clear images which are not too blurry are more effective with Claude 3 models.\n",
    "- **Image Placement**: Claude 3 models work better when images come before text while prompting. However, if the use case requires, images can follow text or can be interpolated with text.\n",
    "- **Image Resolution**: If the image's long edge is more than 1568 pixels, or the image is more than ~1600 tokens, it will first be scaled down, preserving aspect ratio, until it is within size limits. If the input image is too large and needs to be resized, it will increase latency of time-to-first-token, without giving you any additional model performance. Very small images under 200 pixels on any given edge may lead to degraded performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209d2a4-b73a-4dd5-91bd-3a7cd158a40b",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Activity: Text and image search</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"../mlu_utils/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Try prompting the Claude3 models with your own prompts. Explore how the quality and accuracy of responses changes with Image Resolution, Formats, Placement, etc</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e8c7f-2dd3-4752-a19c-b1c409cede03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "\n",
    "############## END OF CODE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d736acb8-40fb-4997-9c12-6715dc73f25d",
   "metadata": {},
   "source": [
    "## <a name=\"6\">Use Cases</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4031d6-196e-4f02-b9d1-3fad099e72a6",
   "metadata": {},
   "source": [
    "### <a name=\"6.1\">1) Explain Images</a>\n",
    "\n",
    "When it comes to describing images, multimodal models can be extremely useful. These models can take an image as input and generate a description that captures the important elements and context within the visual. This has a wide range of applications, including helping people with visual impairments understand images, generating image captions for social media posts, or even creating descriptions for product images in online stores to enhance the user experience and improve accessibility.\n",
    "\n",
    "Let's use a multimodal model to describe the following architecture of a chatbot application using several AWS products. The application uses Bedrock Knowledge Bases to store content from S3 using OpenSearch vector stores. The chatbot application accesses a Lambda function through a Firewall-protected API Gateway which then uses Bedrock to generate a response using the retrieved context from the knowledge base. The response is finally returned back to the chatbot application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5ac87-7dff-457a-970a-6de84fc522ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(\"content/Chatbot-Architecture.png\", width=\"100%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd9d7c3-514a-46fc-9137-da9aee716bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"The image is an AWS architecture diagram to build a chatbot using AWS services. Which service serves as a vector database for the chatbot?\"\n",
    "image_string, image_type = get_base64_encoded_image(\"content/Chatbot-Architecture.png\")\n",
    "\n",
    "response = invoke_claude_3_multimodal(prompt=prompt, images=image_string, image_types=image_type)\n",
    "Markdown(\"<i>\"+response+\"</i>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d285f2-3c50-4780-ba27-38817bd3a458",
   "metadata": {},
   "source": [
    "#### Limitation with GIFs and WebP Content\n",
    "\n",
    "Multimodal models like Claude3 have the ability to process and understand multiple types of input data, such as text, images, and audio. However, when it comes to video or GIF data, there is a limitation in their native ability to analyze individual frames.\n",
    "\n",
    "Videos and GIFs are essentially sequences of images or frames played in rapid succession to create the illusion of motion. Each frame can contain unique visual information, such as the movement of objects, changes in lighting or colors, or the appearance or disappearance of elements.\n",
    "\n",
    "Several multimodal models, including Claude3 models, only parse the first frame of a GIF when prompted with it. This limitation prevents the model from analyzing each frame natively and loses the ability to explain the complete animation.\n",
    "\n",
    "A possible solution could be stacking all the frames into a single image and prompting the model with it. However, that makes the image very complicated, and the model may not be able to analyze the slight differences in each subsequent frame to understand the animation. Recently, there have been a few models that come with the ability to process and analyze video frames to describe videos.\n",
    "\n",
    "Let's try to prompt Claude3 Sonnet with this [GIF](https://commons.wikimedia.org/wiki/File:SdKfz_250-1_(Neu)_(3D-animated).gif):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239e39e-3b55-4839-9264-b9ff7e64871f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(\"content/animated_truck.gif\", width=\"2000%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a775f8-cd55-497a-8db8-3ab5ff5a9966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Explain this GIF.\"\n",
    "image_string, image_type = get_base64_encoded_image(\"content/animated_truck.gif\")\n",
    "\n",
    "response = invoke_claude_3_multimodal(prompt=prompt, images=image_string, image_types=image_type)\n",
    "Markdown(\"<i>\"+response+\"</i>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8204d93-e0c1-41b0-a6c7-b95f976036f4",
   "metadata": {},
   "source": [
    "### <a name=\"6.2\">2) Information Retrieval</a>\n",
    "\n",
    "With documents, these models excel at processing complex layouts and understanding the relationship between text and visual elements. They can extract text from images, understand tables, charts, and graphs, and even interpret hand-written notes. By combining textual and visual understanding, these models provide a comprehensive understanding of the document, making information extraction accurate and contextually aware. \n",
    "\n",
    "The key advantage of multimodal models lies in their ability to bring together disparate sources of information and make inferences that were previously challenging for traditional, single-modality AI systems. \n",
    "\n",
    "Here we will use the [Open-LLM-Leaderboard](https://github.com/VILA-Lab/Open-LLM-Leaderboard) results presented by Myrzakhan et. al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a231499-1f1b-4f2b-978e-64463dcc5812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(\"content/Open-LLM-Leaderboard.png\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32e859f-0f54-44e3-9614-9c890bcf7d7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"The image depicts a leaderboard measuring the performance of various open-source LLMs on evaluation tasks such as ARC, Hellaswag, MMLU, TruthfulQA, Winogrande and GSM8K. \\\n",
    "How did Mistral-7B-v0.1 score on MMLU? Find the model name in the first column and find the corresponding evaluation metric in the MMLU column.\"\"\"\n",
    "image_string, image_type = get_base64_encoded_image(\"content/Open-LLM-Leaderboard.png\")\n",
    "\n",
    "response = invoke_claude_3_multimodal(prompt=prompt, images=image_string, image_types=image_type)\n",
    "Markdown(\"<i>\"+response+\"</i>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c80704-bcaf-4caa-9f4b-5e8a5375b15c",
   "metadata": {},
   "source": [
    "### <a name=\"6.3\">3) Text Extraction and Transcription</a>\n",
    "\n",
    "For applications such as text extraction, these models can identify and extract relevant information from documents, images, or even videos. They excel at recognizing and interpreting text within complex layouts, such as tables, forms, or infographics. By understanding the context and structure of the source material, the models can accurately extract specific data points, summaries, or key information, making it readily available for further analysis or processing.\n",
    "\n",
    "Let's try asking questions about the following [image](https://stock.adobe.com/search/images?filters%5Bcontent_type%3Aphoto%5D=1&filters%5Bcontent_type%3Aillustration%5D=1&filters%5Bcontent_type%3Azip_vector%5D=1&filters%5Bfetch_excluded_assets%5D=1&filters%5Binclude_stock_enterprise%5D=1&filters%5Bcontent_type%3Aimage%5D=1&order=relevance&price%5B%24%5D=1&limit=100&search_page=1&search_type=usertyped&acp=&aco=receipt+handwritten&k=receipt+handwritten&get_facets=0&asset_id=161399068) of a receipt. The image shows that the total amount to be `$20.46` that was authorized on a credit card. Following that, the receipt has the tip amount of `$4.00` written by hand as well as the final amount of `$24.46`, which is also handwritten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b543da8b-ba79-4b0b-ad3c-c8efd86381e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(\"content/Payment-Receipt.jpeg\", width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421339f5-d309-4871-b788-55740143e3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What was the total amount paid by the customer?\"\n",
    "image_string, image_type = get_base64_encoded_image(\"content/Payment-Receipt.jpeg\")\n",
    "\n",
    "response = invoke_claude_3_multimodal(prompt=prompt, images=image_string, image_types=image_type)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91f95a-b7cf-46a1-9eec-b5a0fee7d926",
   "metadata": {},
   "source": [
    "### <a name=\"6.4\">4) Analyze Charts and Graphs</a>\n",
    "\n",
    "Multimodal models are now being leveraged to analyze and interpret charts and graphs, offering a powerful way for understanding complex data visualizations. With their advanced capabilities, these models can provide valuable insights and support data-driven decision-making.\n",
    "\n",
    "These models can identify and classify different types of visualizations, including bar graphs, pie charts, line graphs, and scatter plots, among others. By understanding the structural and visual elements, these models can extract crucial information, such as labels, legends, data values, and their relationships. \n",
    "\n",
    "One of the key strengths of multimodal models lies in their ability to provide contextual understanding. They can interpret chart titles, axis labels, and captions to grasp the underlying narrative of the visualization. By combining this with their text and image analysis capabilities, they can explain the insights presented, identify trends and patterns, and even generate descriptive summaries or highlight key takeaways. \n",
    "\n",
    "Moreover, these models can assist in comparing and contrasting multiple charts, identifying similarities, and relationships between datasets. This capability is particularly useful for spotting anomalies, making predictions, or generating insights that might otherwise be difficult to discern. By bringing together text, image, and even tabular data interpretation, multimodal models enhance the accessibility and comprehension of chart-based information, enabling users to make more informed decisions.\n",
    "\n",
    "We will use the following [chart](https://stock.adobe.com/search/images?k=global+surface+temperature+graph&search_type=usertyped&asset_id=543960453) illustrating how global average temperatures have changed over the years compared with the average global temperature of the mid-20th century. The chart is plotted at intervals of 20 years. The chart shows that before 1950s, average global temperatures were much lower than that of mid-20th century temperatures. However, the average global temperatures have steadily been increasing since then with an increase of 0.89 Celsius higher than the temperatures in mid-20th centure as recorded in 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa5dda-0bc6-4b4e-9005-dedf393ca539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(\"content/chart-data.jpeg\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a9cd30-a1dc-4d65-813d-8a9037d6a90a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Explain the trend of global temperature between 1940 and 1980.\"\n",
    "image_string, image_type = get_base64_encoded_image(\"content/chart-data.jpeg\")\n",
    "\n",
    "response = invoke_claude_3_multimodal(prompt=prompt, images=image_string, image_types=image_type)\n",
    "Markdown(\"<i>\"+response+\"</i>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeba98e-f0f1-48be-ae9e-82a14941c145",
   "metadata": {},
   "source": [
    "### <a name=\"6.5\">5) Improve Accessibility</a>\n",
    "\n",
    "Multimodal models are revolutionizing the field of accessibility by generating descriptive content for images and scenes, making digital content more inclusive for all users. \n",
    "\n",
    "One of the key applications of these models is in the creation of alt-text for images. By analyzing the visual content, multimodal models can generate descriptive alt-text that conveys the image's key information. This alt-text is crucial for individuals with visual impairments, as it allows them to understand the context and significance of images through assistive technologies like screen readers. The models can identify objects, recognize scenes, and describe the image's composition, ensuring a rich and detailed portrayal. \n",
    "\n",
    "Additionally, these models can provide detailed scene descriptions, offering a narrative-like explanation of what is happening in a given image or video frame. This goes beyond simple object recognition by capturing the atmosphere, actions, and relationships between elements. Scene descriptions are particularly beneficial for those with visual or cognitive disabilities, as they paint a comprehensive mental picture, enhancing their understanding and engagement with the content. We will further explore this this application in Lab5.\n",
    "\n",
    "Let's consider the following [image](https://stock.adobe.com/search/images?filters%5Bcontent_type%3Aphoto%5D=1&filters%5Bcontent_type%3Aillustration%5D=1&filters%5Bcontent_type%3Azip_vector%5D=1&filters%5Bcontent_type%3Avideo%5D=0&filters%5Bcontent_type%3Atemplate%5D=0&filters%5Bcontent_type%3A3d%5D=0&filters%5Bcontent_type%3Aaudio%5D=0&filters%5Bfetch_excluded_assets%5D=1&filters%5Binclude_stock_enterprise%5D=1&filters%5Bis_editorial%5D=0&filters%5Bfree_collection%5D=0&filters%5Bcontent_type%3Aimage%5D=1&order=relevance&price%5B%24%5D=1&limit=100&search_page=1&native_visual_search=672d2d77b5e73&similar_content_id=&model_id=&serie_id=&find_similar_by=all&search_type=usertyped&acp=&get_facets=0&asset_id=474696794) representing a few keyboard shortcuts along with common keys used for such shortcuts such as 'Alt', 'Ctrl'. Two such shortcuts representing in the image are 'Ctrl' + 'c' and 'Ctrl' + 'v' which are commonly used to copy and past items in Windows devices respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddeb48-d6ee-4c09-8259-d474d61fb571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Image(\"content/shortcuts.jpeg\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235b81a4-3b9b-441e-a747-feb005949ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Produce alt-text for the given image following the guidelines.\"\n",
    "guidelines = \"\"\" Guidelines for alt-text:\n",
    "- Be succinct but convey the same information a sighted customer receives from the image.\n",
    "- Keep alt text length to 155 characters or less for SEO reasons.\n",
    "- Don't duplicate text already in the content.\n",
    "- Avoid phrases like \"image of ...\" or \"graphic of ...\" to describe the image. Screen readers already indicate that alt text is part of an image.\n",
    "- Include punctuation when needed. When screen readers encounter punctuation, they pause before continuing.\n",
    "- Avoid using all-caps in alt text. Some screen readers read capital letters individually.\n",
    "\"\"\"\n",
    "\n",
    "image_string, image_type = get_base64_encoded_image(\"content/shortcuts.jpeg\")\n",
    "\n",
    "response = invoke_claude_3_multimodal(prompt=prompt + \"\\n\" + guidelines, images=image_string, image_types=image_type)\n",
    "Markdown(\"<i>\"+response+\"</i>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fac457-69e8-404c-ba0e-18449b581d18",
   "metadata": {},
   "source": [
    "### <a name=\"6.6\">6) Complex Reasoning and Analysis of Documents</a>\n",
    "\n",
    "Multimodal models are effective tools for parsing, analyzing, and retrieving information from documents such as PDFs, offering a seamless way to extract valuable insights from a variety of sources. \n",
    "\n",
    "One of the key strengths of these models is their ability to process and understand structured and unstructured data within PDFs. They can parse through tables, forms, charts, and free-form text, recognizing and extracting relevant information. Whether it's extracting specific data points, identifying patterns, or understanding contextual relationships, multimodal models provide a comprehensive understanding of the document's content. \n",
    "\n",
    "Another advantage is their capability to interpret and analyze visual elements, such as graphs, diagrams, and images, often found in PDFs. By combining image recognition with textual analysis, these models can explain and provide insights from visual representations, making the information more accessible and actionable. This is particularly useful for industries that rely on visual data, such as engineering, architecture, or scientific research. \n",
    "\n",
    "By leveraging the power of multimodal AI, the process of parsing, analyzing, and retrieving information from PDFs becomes more efficient and robust. Models like Claude3 enable users to unlock valuable insights, make data-driven decisions, and easily access the information they need, regardless of the document's format or structure. This enhances productivity, facilitates knowledge sharing, and improves the overall user experience when working with PDF documents.\n",
    "\n",
    "Let's try to analyze the following [PDF](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-text-models.html) about Amazon's Titan Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64236e3c-bc74-413d-ba7e-2c4425d68874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IFrame(\"content/Titan Models-Docs.pdf\", width=600, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8438248e-b412-441e-9c16-e893b3a8a79c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to convert pdf to images\n",
    "def pdf2img(pdf_path, pdf_pages_dir):\n",
    "    import pypdfium2 as pdfium\n",
    "    from PIL import Image as pilimage\n",
    "    \n",
    "    pdf = pdfium.PdfDocument(pdf_path)\n",
    "\n",
    "    if not os.path.exists(pdf_pages_dir):\n",
    "        os.makedirs(pdf_pages_dir)\n",
    "\n",
    "    resolution = pdf.get_page(0).render().to_numpy().shape\n",
    "    scale = 1 if resolution[0] >= 1620 or resolution[1] >=1620 else 300/72\n",
    "    n_pages = len(pdf)\n",
    "    for page_number in range(n_pages):\n",
    "        page = pdf.get_page(page_number)\n",
    "        pil_image = page.render(\n",
    "            scale=scale,\n",
    "            rotation=0,\n",
    "            crop=(0, 0, 0, 0),\n",
    "            may_draw_forms=False,\n",
    "            fill_color=(255, 255, 255, 255),\n",
    "            draw_annots=False,\n",
    "            grayscale=False,\n",
    "        ).to_pil()\n",
    "        width, height = pil_image.size\n",
    "        pil_image = pil_image.resize((width//2*2, height//2*2), pilimage.LANCZOS)\n",
    "        pil_image.save(os.path.join(pdf_pages_dir, \"page_{}.png\".format(str(page_number).zfill(3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522967d2-082d-476b-b16d-070249792350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyse_pdf(prompt, pdf_path):\n",
    "    \n",
    "    pdf_pages_dir = \"pdf_pages\"\n",
    "    pdf2img(pdf_path=pdf_path, pdf_pages_dir=pdf_pages_dir)\n",
    "    \n",
    "    # Check whether the pdf has less than or equal to 5 pages\n",
    "    if len(os.listdir(pdf_pages_dir)) > 5:\n",
    "        raise ValueError(\"For Claude 3, only 5 page PDFs are supported.\")\n",
    "        \n",
    "    image_strings, image_types = get_base64_encoded_image([os.path.join(pdf_pages_dir, file) for file in sorted(os.listdir(pdf_pages_dir))])\n",
    "    response = invoke_claude_3_multimodal(prompt=prompt, images=image_strings, image_types=image_types)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f99bd9-cfe8-4864-b56f-f3d625d09d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf_path = \"content/Titan Models-Docs.pdf\"\n",
    "prompt = \"What is the 'max token' value for Titan Premier?\"\n",
    "\n",
    "response = analyse_pdf(prompt, pdf_path)\n",
    "Markdown(\"<i>\"+response+\"</i>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef93f5c2-335f-4b9e-95b1-f3b51eff1882",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Activity: Text and image search</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"../mlu_utils/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Analyzing PDFs this way can be quite challenging as the model is attempting to process multiple images which contain a lot of text. <br>\n",
    "        Try a few more prompts to explore the model's capability in answering specific questions about the content in the PDF.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c6b472-d22a-4d19-a9fa-8540a000d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "\n",
    "############## END OF CODE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4803f0a8-ab7e-4ac7-9503-745eb434846a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### <a name=\"7\">Quiz Questions</a>\n",
    "\n",
    "Well done completing the lab! Now, it's time for a brief knowledge assessment.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"../mlu_utils/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Answer the following questions to test your understanding of using MLLMs for inference.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65f3d9-6cbb-4be1-82b5-5ad782605595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from mlu_utils.quiz_questions import *\n",
    "\n",
    "lab5_question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcb9e55-2e9a-415c-ae4d-fb396cc6de83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lab5_question2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce4f4ad-79bc-413a-80cc-dd333b52abf7",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "\n",
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"../mlu_utils/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

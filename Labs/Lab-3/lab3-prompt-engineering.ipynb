{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56d9c9e",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"../mlu_utils/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "# <a name=\"0\">Machine Learning University</a>\n",
    "## <a name=\"0\">Lab 3: Prompt Engineering</a>\n",
    "\n",
    "Prompt engineering is the process of constructing and refining input prompts to improve the quality of generated responses from language models. It's an iterative process that requires experimentation to find the optimal approach for a given problem.\n",
    "\n",
    "Key strategies for effective prompt engineering include:\n",
    "- Writing clear and specific instructions\n",
    "- Highlighting or specifying the relevant parts of the prompt\n",
    "- Adding details or restrictions to guide the model's output\n",
    "- Instructing the model to follow a step-by-step approach for complex tasks\n",
    "\n",
    "In this lab, we'll use the Amazon Titan Text G1 - Premier model to demonstrate various prompt engineering techniques across different machine learning tasks.\n",
    "    \n",
    "1. <a href=\"#1\">Installing libraries</a>\n",
    "2. <a href=\"#2\">Example problems</a>\n",
    "    1. <a href=\"#2.1\">Text summarization</a>\n",
    "    2. <a href=\"#2.2\">Question-answering</a>\n",
    "    3. <a href=\"#2.3\">Text generation</a>\n",
    "    4. <a href=\"#2.4\">In-context learning</a>\n",
    "    5. <a href=\"#2.5\">Chain-of-thought</a>\n",
    "3. <a href=\"#3\">Quizzes</a>\n",
    "\n",
    "### In this lab, we will cover topics such as:\n",
    "- Introduction to Prompt Engineering\n",
    "- Text Summarization\n",
    "- Question Answering \n",
    "- Text Generation\n",
    "- In-context Learning: Zero-shot, One-shot, and Few-shot Learning\n",
    "- Chain of Thought\n",
    "    \n",
    "Let's begin by setting up our environment and connecting to the Bedrock service.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd69bd",
   "metadata": {},
   "source": [
    "### <a name=\"1\">Installation of libraries </a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We first install recent versions of boto3 and botocore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf459059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6beb1cd",
   "metadata": {},
   "source": [
    "We access the Bedrock service through boto3 by providing the service name, region name and endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d0d96e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json, boto3\n",
    "\n",
    "session = boto3.session.Session()\n",
    "\n",
    "bedrock_inference = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=session.region_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0a8d86",
   "metadata": {},
   "source": [
    "Let's specify the API parameters. We will use the __Amazon Titan Text G1 - Premier__ model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bc332b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send_prompt(prompt_data, temperature=0.0, top_p=0.5, max_token_count=1000):\n",
    "\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"inputText\": prompt_data,\n",
    "            \"textGenerationConfig\": {\n",
    "                \"temperature\": temperature,\n",
    "                \"topP\": top_p,\n",
    "                \"maxTokenCount\": max_token_count,\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    modelId = \"amazon.titan-text-premier-v1:0\"\n",
    "\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    response = bedrock_inference.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "\n",
    "    return response_body[\"results\"][0][\"outputText\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69da370b",
   "metadata": {},
   "source": [
    "## <a name=\"2\">Example problems</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e77d1a",
   "metadata": {},
   "source": [
    "### <a name=\"2.1\">1) Text summarization:</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "With text summarization, the main purpose is to create a shorter version of a given text while preserving the relevant information in it. \n",
    "\n",
    "We use the following text from the [sustainability section](https://sustainability.aboutamazon.com/environment/renewable-energy) of [about.amazon.com](https://www.aboutamazon.com/).\n",
    "\n",
    "\n",
    "<p style=\"font-size:12pt;\">\n",
    "    \"In 2021, we reached 85% renewable energy across our business. Our first solar projects in South Africa and the United Arab Emirates came online, and we announced new projects in Singapore, Japan, Australia, and China. Our projects in South Africa and Japan are the first corporate-backed, utility-scale solar farms in these countries. We also announced two new offshore wind projects in Europe, including our largest renewable energy project to date. As of December 2021, we had enabled more than 3.5 gigawatts of renewable energy in Europe through 80 projects, making Amazon the largest purchaser of renewable energy in Europe.\"\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83037ba5",
   "metadata": {},
   "source": [
    "Let's start with the first summarization example below. We pass this text as well as the instruction to summarize it. The instruction part of the prompt becomes __The following is a text about Amazon. Summarize this:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa74f86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"The following is a text about Amazon. Summarize this:  \\\n",
    "Text: In 2021, we reached 85% renewable energy across our business.\\\n",
    "Our first solar projects in South Africa and the United Arab Emirates\\\n",
    "came online, and we announced new projects in Singapore, Japan, \\\n",
    "Australia, and China. Our projects in South Africa and Japan are \\\n",
    "the first corporate-backed, utility-scale solar farms in these \\\n",
    "countries. We also announced two new offshore wind projects in \\\n",
    "Europe, including our largest renewable energy project to date.\\\n",
    "As of December 2021, we had enabled more than 3.5 gigawatts of \\\n",
    "renewable energy in Europe through 80 projects, making Amazon \\\n",
    "the largest purchaser of renewable energy in Europe.\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a344304f",
   "metadata": {},
   "source": [
    "Nice. This text is shorter. We can set the desired lenght of the summary by adding more constraints to the instructions. Let's create a one-sentence summary of this text. The instruction part of the prompt becomes the following: __The following is a text about Amazon. Summarize it in one sentence.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cbefed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"The following is a text about Amazon. Summarize it in one sentence. \\\n",
    "Text: In 2021, we reached 85% renewable energy across our business.\\\n",
    "Our first solar projects in South Africa and the United Arab Emirates\\\n",
    "came online, and we announced new projects in Singapore, Japan, \\\n",
    "Australia, and China. Our projects in South Africa and Japan are \\\n",
    "the first corporate-backed, utility-scale solar farms in these \\\n",
    "countries. We also announced two new offshore wind projects in \\\n",
    "Europe, including our largest renewable energy project to date.\\\n",
    "As of December 2021, we had enabled more than 3.5 gigawatts of \\\n",
    "renewable energy in Europe through 80 projects, making Amazon \\\n",
    "the largest purchaser of renewable energy in Europe.\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f078f7a",
   "metadata": {},
   "source": [
    "Nice! The model generated a one-sentence summary.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34b05d",
   "metadata": {},
   "source": [
    "### <a name=\"2.2\">2) Question Answering</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In Question Answering problem, a Machine Learning model answers some questions using some provided context. Here as context, we will use the previous text about Amazon's sustainability efforts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6452a672",
   "metadata": {},
   "source": [
    "The first question is asking about the names of the countries mentioned in the text. \n",
    "\n",
    "The instruction section of the prompt is __What are the names of the countries in the following text?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee90d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"What are the names of the countries in the following text? \\\n",
    "Text: In 2021, we reached 85% renewable energy across our business.\\\n",
    "Our first solar projects in South Africa and the United Arab Emirates\\\n",
    "came online, and we announced new projects in Singapore, Japan, \\\n",
    "Australia, and China. Our projects in South Africa and Japan are \\\n",
    "the first corporate-backed, utility-scale solar farms in these \\\n",
    "countries. We also announced two new offshore wind projects in \\\n",
    "Europe, including our largest renewable energy project to date.\\\n",
    "As of December 2021, we had enabled more than 3.5 gigawatts of \\\n",
    "renewable energy in Europe through 80 projects, making Amazon \\\n",
    "the largest purchaser of renewable energy in Europe.\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb290be",
   "metadata": {},
   "source": [
    "Nice. We get all of the geographical places mentioned in the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c017ccf",
   "metadata": {},
   "source": [
    "Let's try to learn something specific about the document. For example, the amount of gigawatts that the project in Europe enabled. \n",
    "\n",
    "The instruction section of the prompt is __How many gigawatts of energy did Amazon enable in Europe according to the following text?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008ff876",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"How many gigawatts of energy did Amazon enable in \\\n",
    "Europe according to the following text? \\\n",
    "Text: In 2021, we reached 85% renewable energy across our business.\\\n",
    "Our first solar projects in South Africa and the United Arab Emirates\\\n",
    "came online, and we announced new projects in Singapore, Japan, \\\n",
    "Australia, and China. Our projects in South Africa and Japan are \\\n",
    "the first corporate-backed, utility-scale solar farms in these \\\n",
    "countries. We also announced two new offshore wind projects in \\\n",
    "Europe, including our largest renewable energy project to date.\\\n",
    "As of December 2021, we had enabled more than 3.5 gigawatts of \\\n",
    "renewable energy in Europe through 80 projects, making Amazon \\\n",
    "the largest purchaser of renewable energy in Europe.\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ec72d1",
   "metadata": {},
   "source": [
    "Nice. We were able to extract that information and return in the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bf7248",
   "metadata": {},
   "source": [
    "Let's try another example, this time without a question that doesn't need an input text. An explicit input may not be necessary for some questions. For example, we can ask some general questions like below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276109b0",
   "metadata": {},
   "source": [
    "__How many months are there in a year?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e989d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"How many months are there in a year?\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc5e757",
   "metadata": {},
   "source": [
    "__How many meters are in a mile?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80010b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"How many meters are in a mile?\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3b2509",
   "metadata": {},
   "source": [
    "__What is the result when you add up 2 and 9?__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06f9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"What is the result when you add up 2 and 9?\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e62ad2",
   "metadata": {},
   "source": [
    "The answers above are correct.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f74f1",
   "metadata": {},
   "source": [
    "### <a name=\"2.3\">3) Text Generation</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3e2fac",
   "metadata": {},
   "source": [
    "Text generation is one of the common use cases for Large Language Models. The main purpose is to generate some high quality text considering a given input. We will cover a few examples here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc351f35",
   "metadata": {},
   "source": [
    "__Customer service example:__\n",
    "\n",
    "Let's start with a customer feedback example. Assume we want to write an email to a customer who had some problems with a product that they purchased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69189eaa",
   "metadata": {},
   "source": [
    "__Write an email response from Example Corp company customer service \\\n",
    "based on the following email that was received from a customer.__\n",
    "\n",
    "__Customer email: \"I am not happy with this product. I had a difficult \\\n",
    "time setting it up correctly because the instructions do not cover all \\\n",
    "the details. Even after the correct setup, it stopped working after \\\n",
    "a week.\"__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c96d970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Write an email response from Example Corp company customer service \\\n",
    "based on the following email that was received from a customer.\n",
    "\n",
    "Customer email: \"I am not happy with this product. I had a difficult \\\n",
    "time setting it up correctly because the instructions do not cover all \\\n",
    "the details. Even after the correct setup, it stopped working after \\\n",
    "a week.\" \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c10d06d",
   "metadata": {},
   "source": [
    "Nice! The generated text asks customer to provide more details to resolve the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cee81e",
   "metadata": {},
   "source": [
    "__Generating product descriptions:__\n",
    "\n",
    "We can use generative AI to write creative product descriptions for our products. In the example below, we create three product descriptions for a sunglasses product.\n",
    "\n",
    "__Product: Sunglasses.  \\\n",
    "Keywords: polarized, style, comfort, UV protection. \\\n",
    "List three variations of a detailed product \\\n",
    "description for the product listed above, each \\\n",
    "variation of the product description must \\\n",
    "use at least two of the listed keywords.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dcadac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Product: Sunglasses.  \\\n",
    "Keywords: polarized, style, comfort, UV protection. \\\n",
    "List three different product descriptions \\\n",
    "for the product listed above using \\\n",
    "at least two of the provided keywords.\"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2927d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d0b215",
   "metadata": {},
   "source": [
    "### <a name=\"2.4\">4) In-context learning</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63277779",
   "metadata": {},
   "source": [
    "As pre-trained large language models learn from large and diverse data sources, they tend to build a holistic view of languages and text. This advantage allows them to learn from some input-output pairs present within the input texts. \n",
    "\n",
    "In this section, we will explain this __\"in-context\"__ learning capability with some examples. Depending on the level of information presented to the model, we can use zero-shot, one-shot or few-shot learning. We start with the most extreme case, no information presented to the model. This is called __\"zero-shot-learning\"__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5be1f4",
   "metadata": {},
   "source": [
    "#### Zero-shot learning:\n",
    "Assume the model is given a translation task and an input word.\n",
    "\n",
    "__Translate English to Spanish \\\n",
    " cat ==>__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870253fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Translate the following word from English to Spanish \\\n",
    "word: cat \\\n",
    "translation: \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8e8f5",
   "metadata": {},
   "source": [
    "Correctly translated to Spanish. Let's try something different in the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0e14c",
   "metadata": {},
   "source": [
    "#### One-shot learning:\n",
    "We can give the model one example and let it learn from the example to solve a problem. Below, we provide an example sentence about a cat and the model completes the second sentence about a table in a similar way.\n",
    "\n",
    "__Answer the last question \\\n",
    "question: what is a cat? \\\n",
    "answer: cat is an animal \\\n",
    "\\##  \\\n",
    "last question: what is a car?\\\n",
    "answer: car is__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25234f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Answer the last question \\\n",
    "question: what is a cat? \\\n",
    "answer: cat is an animal \\\n",
    "## \\\n",
    "last question: what is a car? \\\n",
    "answer: car is \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125436a0",
   "metadata": {},
   "source": [
    "It worked very well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37137bf",
   "metadata": {},
   "source": [
    "#### Few-shot learning:\n",
    "We can give the model multiple examples to learn from. Providing more examples can help the model produce more accurate results. Let's also change the style of the example answers by adding some __negation__ to them.\n",
    "\n",
    "__Answer the last question \\\n",
    "question: what is a car? \\\n",
    "answer: car is not an animal \\\n",
    "\\## \\\n",
    "question: what is a cat? \\\n",
    "answer: cat is not a vehicle \\\n",
    "\\## \\\n",
    "last question: what is a shoe? \\\n",
    "answer: shoe is__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204095c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Answer the last question\n",
    "question: what is a car?\n",
    "answer: car is not an animal\n",
    "##\n",
    "question: what is a cat?\n",
    "answer: cat is not a vehicle\n",
    "##\n",
    "last question: what is a shoe?\n",
    "answer: shoe is \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de15b9",
   "metadata": {},
   "source": [
    "The response picked up the overall style very well. See that it responded starting with \"not\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed4732d",
   "metadata": {},
   "source": [
    "We can increase the __temperature__ to get different responses. Let's try that below.\n",
    "Try running the cell below multiple times to obtain different answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2696e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Answer the last question \n",
    "question: what is a car?\n",
    "answer: car is not an animal\n",
    "##\n",
    "question: what is a cat?\n",
    "answer: cat is not a vehicle\n",
    "##\n",
    "last question: what is a shoe?\n",
    "answer: shoe is \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, top_p=1.0, temperature=0.85))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0aeadd",
   "metadata": {},
   "source": [
    "Let's try one more example. This time we remove the instruction and try to complete the last sentence.\n",
    "\n",
    "__question: what is a cat? \\\n",
    "answer: cat is a domesticated wild animal that belongs to the Felidae family. \\\n",
    "\\##  \\\n",
    "question: what is a car? \\\n",
    "answer: car is a vehicle with wheels that is used for transportation. \\\n",
    "\\##  \\\n",
    "last question: what is a shoe?\\\n",
    "answer: shoe is__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7eb41a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\" \n",
    "question: what is a cat?\n",
    "answer: cat is a domesticated wild animal that belongs to the Felidae family.\n",
    "##\n",
    "question: what is a car?\n",
    "answer: car is a vehicle with wheels that is used for transportation.\n",
    "##\n",
    "question: what is a shoe?\n",
    "answer: shoe is \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54478869",
   "metadata": {},
   "source": [
    "It worked again. The model nicely followed the provided pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ccf36d",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3d731",
   "metadata": {},
   "source": [
    "### <a name=\"2.5\">5) Chain of thought concept</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d8f485",
   "metadata": {},
   "source": [
    "Chain of thought concept breaks down a problem into a series of intermediate reasoning steps. This way of thinking has significantly improved the quality of the outputs generated by the Large Language Models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3952ca8",
   "metadata": {},
   "source": [
    "Here is the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36803d88",
   "metadata": {},
   "source": [
    "### Example without Chain-of-thought (CoT)\n",
    "\n",
    "__Answer the following question.__\n",
    "\n",
    "__Question: When I was 16, my sister was half of my age.__ \\\n",
    "__Now, I’m 42. How old is my sister now?__ \\\n",
    "\n",
    "__Answer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d850cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Answer the following question.\n",
    "\n",
    "Question: When I was 16, my sister was half of my age. \\\n",
    "Now, I’m 42. How old is my sister now?\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a73c54",
   "metadata": {},
   "source": [
    "The answer is __incorrect__! This is not a big surprise. Many Large Language Models make these types of mistakes. In this case, the model skipped a few steps to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8127e21c",
   "metadata": {},
   "source": [
    "### Example with zero-shot CoT\n",
    "It is also possible to elicit reasoning from LLMs in a **zero-shot** situation without needing to provide one-shot examples. An explicit instruction for the model to ”**think step by step**“ might help the LLM find the right solution. This approach is called zero-shot chain of thought.\n",
    "\n",
    "__Answer the following question.__\n",
    "\n",
    "__Question: When I was 16, my sister was half of my age.__ \\\n",
    "__Now, I’m 42. How old is my sister now?__ \\\n",
    "\n",
    "__Let's think step by step and describe all steps.__ \\\n",
    "\n",
    "__Answer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935b66f-a34a-468a-bd98-c5a5e92445ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Answer the following question. \n",
    "\n",
    "Question: When I was 16, my sister was half of my age. \\\n",
    "Now, I’m 42. How old is my sister now?\n",
    "\n",
    "Let's think step by step and describe all steps.\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e54cc5d",
   "metadata": {},
   "source": [
    "The model answers correctly! You can also choose to provide the model with examples to guide its reasoning approach. This can enable the model to adopt different and potentially more robust reasoning strategies based on the examples provided.\n",
    "\n",
    "### One-shot CoT\n",
    "Allowing the model to **think step-by-step** allowed the model to reason through the problem and come to the correct answer.\n",
    "Let's try another idea. As we have seen in the __in-context__ learning topic, LLMs tend to learn from the provided inputs and apply those learnings to another problems. Here, we will first provide the step-by-step solution for the problem with different numbers and then ask the model to solve the original problem. Since we are showing the model to reason by showing it one example, this approach is called one-shot chain of thought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1690cdd1",
   "metadata": {},
   "source": [
    "__Answer the following question:__ \n",
    "\n",
    "__Question: When I was 10, my sister was half of my age.__ \\\n",
    "__Now, I’m 70. How old is my sister now?__\n",
    "\n",
    "__Answer: When I was 10 years old, my sister was half of my age.__ \\\n",
    "__So, the age of the sister at that time = 10/2 = 5__ \\\n",
    "__This implies that the sister is 5 years younger.__ \\\n",
    "__Now, when I’m 70 years and age of sister = 70 - 5__ \\\n",
    "__Age of sister = 65.__\n",
    "\n",
    "__Question: When I was 16, my sister was half of my age.__ \\\n",
    "__Now I’m 42. How old is my sister now?__\n",
    "\n",
    "__Answer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8508faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = \"\"\"Answer the following question.\n",
    "\n",
    "Question: When I was 10, my sister was half of my age. \\\n",
    "Now, I’m 70. How old is my sister now?\n",
    "\n",
    "Answer: When I was 10 years old, my sister was half of my age. \\\n",
    "So, the age of the sister at that time = 10/2 = 5 \\\n",
    "This implies that the sister is 5 years younger. \\\n",
    "Now, when I’m 70 years and age of sister = 70 - 5 \\\n",
    "Age of sister = 65. \\\n",
    "\n",
    "Question: When I was 16, my sister was half of my age. \\\n",
    "Now I’m 42. How old is my sister now?\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "print(send_prompt(prompt_data, temperature=0.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c485f3b5",
   "metadata": {},
   "source": [
    "The model followed the given example and applied the same steps to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba805d67-9ee8-4a56-ac86-900fda2231a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### <a name=\"3\">Quiz Questions</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Well done on completing the lab! Now, it's time for a brief knowledge assessment.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"../mlu_utils/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Answer the following questions to test your understanding of basic prompt engineering practices.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6896de69-f168-4a91-ba9f-aebd0419bb75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from mlu_utils.quiz_questions import *\n",
    "\n",
    "lab3_question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd894270-76bd-48d9-9b61-b02b1f474eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lab3_question2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987e4ed8-075c-4c59-9345-60899b392130",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "\n",
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"../mlu_utils/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

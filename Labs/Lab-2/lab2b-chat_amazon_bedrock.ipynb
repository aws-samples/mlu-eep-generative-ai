{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56d9c9e",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"../mlu_utils/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">Machine Learning University</a>\n",
    "## <a name=\"0\">Lab2b: Amazon Bedrock with LangChain</a>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "    \n",
    "[__LangChain__](https://python.langchain.com/docs/introduction/) is a popular open source framework to develop applications with Large Language Models. Recent versions of LangChain started to support Amazon Bedrock models.\n",
    "\n",
    "In this notebook, we will build __a simple chatbot__ using Amazon's __Amazon Titan Text G1 - Premier__ with Amazon Bedrock. Let's start!\n",
    "\n",
    "1. <a href=\"#1\">Installation and API calls </a>\n",
    "2. <a href=\"#2\">Starting the conversation</a>\n",
    "3. <a href=\"#3\">Quizzes</a>\n",
    "\n",
    "---\n",
    "    \n",
    "### In this lab, we will cover topics such as:\n",
    "- Setting up and accessing the Bedrock service using boto3\n",
    "- Exploring available Large Language Models (LLMs) in Bedrock\n",
    "- Performing Bedrock API calls with various customization options\n",
    "- Understanding and manipulating model parameters for text generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636e840f",
   "metadata": {},
   "source": [
    "### <a name=\"1\">Installation and API calls </a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fd69bd",
   "metadata": {},
   "source": [
    "Installing a recent version of LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf459059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a842a-3ced-4f3a-8c10-87fd23ba1890",
   "metadata": {},
   "source": [
    "Code that will suppress deprecation warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78c754-a068-451c-9a8b-eed6018ef7fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e9b02f",
   "metadata": {},
   "source": [
    "We import the Bedrock module first and use the __Amazon Titan Text G1 - Premier__ from it. We can pass model parameters at this point. For example we set the temperature to zero and maximum number of tokens in the text response to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c056c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "session = boto3.session.Session()\n",
    "\n",
    "llm = BedrockLLM(\n",
    "    model_id=\"amazon.titan-text-premier-v1:0\",\n",
    "    model_kwargs={\"temperature\": 0.0, \"maxTokenCount\": 500},\n",
    "    region_name=session.region_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9fca4",
   "metadata": {},
   "source": [
    "We can also set a certain prompt template. Below, we create a slightly different version of the default template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7532af8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. \\\n",
    "If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Question: {input}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70328c3",
   "metadata": {},
   "source": [
    "Once we have the LLM and the prompt template, we can start a conversation chain. Inside the function, we provide the LLM and set a few other parameters. \n",
    "* __verbose__ prints the conversation history during the chat\n",
    "* __memory__ is responsible for storing the conversation history\n",
    "* Inside __ConversationBufferMemory()__, we can also set different names for the AI and user through __ai_prefix__ and __human_prefix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ed00f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory(ai_prefix=\"Answer\", human_prefix=\"Question\"),\n",
    "    prompt=prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d263c4",
   "metadata": {},
   "source": [
    "Let's print out our conversation template. This is a general template for conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aebf49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "default_prompt_template = conversation.prompt.template\n",
    "\n",
    "Markdown(default_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231b4039",
   "metadata": {},
   "source": [
    "Let's start the conversation!  We send our message by calling the __predict()__ function with our text. As we set the __verbose__ parameter __true__ earlier, history of the conversation will be printed out first. Then, we will see the response of the chatbot.\n",
    "\n",
    "### <a name=\"2\">Starting the conversation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "### First message\n",
    "\n",
    "Let's start with asking the AI if they are familiar with machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313597f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Markdown(conversation.predict(input=\"Hello! Are you familiar with Machine Learning?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35590eb3",
   "metadata": {},
   "source": [
    "#### Second message\n",
    "\n",
    "Next, we are specifically interested in LLMs. Let's ask about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461c88bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Markdown(\n",
    "    conversation.predict(\n",
    "        input=\"Can you list a few applications of Large Language Models?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8b506",
   "metadata": {},
   "source": [
    "#### Third message\n",
    "\n",
    "We ask about some recent developments in the field to learn more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf84eeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Markdown(conversation.predict(input=\"What are some recent developments in LLMs?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2fb777",
   "metadata": {},
   "source": [
    "#### The last message\n",
    "\n",
    "At the end, we thank and end the conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c8ae1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Markdown(conversation.predict(input=\"Nice. Thanks.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db7821-9f76-4274-9911-a62df82fbbc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### <a name=\"3\">Quiz Questions</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Well done on completing the lab! Now, it's time for a brief knowledge assessment.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"../mlu_utils/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Answer the following questions to test your understanding of using MLLMs for inference.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b30925-8d0c-41d3-83f3-24e12f264dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from mlu_utils.quiz_questions import *\n",
    "\n",
    "lab2b_question1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bbc06e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Thank you!\n",
    "\n",
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"../mlu_utils/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

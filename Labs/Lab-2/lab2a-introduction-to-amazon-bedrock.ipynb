{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56d9c9e",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"../mlu_utils/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">Machine Learning University</a>\n",
    "## <a name=\"0\">Lab2a: Amazon Bedrock API Walkthrough</a>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "    \n",
    "Amazon Bedrock is a fully managed service that makes foundation models (FMs) from Amazon and third-party model providers easily accessible through an API. This notebook covers the Amazon Bedrock API using SDK for Python (Boto3).\n",
    "    \n",
    "1. <a href=\"#1\">Installation and API calls</a>\n",
    "1. <a href=\"#2\">Accessing the Bedrock service using the SDK for Python</a>\n",
    "2. <a href=\"#3\">Bedrock API Call</a>\n",
    "3. <a href=\"#4\">Quizzes</a>\n",
    "\n",
    "In this lab, we will cover topics such as:\n",
    "- Setting up and accessing the Bedrock service using Boto3\n",
    "- Exploring available Large Language Models (LLMs) in Bedrock\n",
    "- Performing Bedrock API calls with various customization options\n",
    "- Understanding and manipulating model parameters for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d627c33-2f13-4893-9281-7da39e2c1949",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### <a name=\"1\">Installation of libraries </a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28483e31",
   "metadata": {},
   "source": [
    "We first install recent versions of boto3 and botocore (defined in a file called `requirements.txt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a036aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6beb1cd",
   "metadata": {},
   "source": [
    "We can access the Bedrock service through boto3 by providing the service name, region name and endpoint URL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba1fb13",
   "metadata": {},
   "source": [
    "### <a name=\"2\">Accessing the Bedrock service using the SDK for Python</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266edd88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json, boto3\n",
    "\n",
    "session = boto3.session.Session()\n",
    "\n",
    "bedrock = session.client(\n",
    "    service_name=\"bedrock\",\n",
    "    region_name=session.region_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce0b608",
   "metadata": {},
   "source": [
    "Let's take a look at the available Large Language Models (LLMs) in Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c3ba5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock.list_foundation_models()[\"modelSummaries\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d79958",
   "metadata": {},
   "source": [
    "We use the service name 'bedrock-runtime' for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb6c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For inference\n",
    "bedrock_inference = session.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65e5fd",
   "metadata": {},
   "source": [
    "Model IDs can be used to select a specific model in the API calls. \n",
    "\n",
    "This demo is focusing on the __Amazon Titan Text G1 - Premier__ model from __Amazon__. We will use the model with id: `amazon.titan-text-premier-v1:0`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb64cb",
   "metadata": {},
   "source": [
    "### <a name=\"3\">Bedrock API Call</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Bedrock API has the following parameters:\n",
    "* __`body`:__ The message body that includes the input text and model parameters. Model parameters help customize the models and the generated outputs.\n",
    "* __`modelId`:__ Identifier of the model. We can pick a model from the list of models printed in the previous code block.\n",
    "* __`accept`:__ The desired type of the inference body in the response.\n",
    "* __`contentType`:__ The type of the input data in the request body.\n",
    "\n",
    "<br>\n",
    "\n",
    "__Model parameters:__\n",
    "\n",
    "These parameters are provided within the body of the API call. Basically, we can control the randomness and the length of the generated sequences. \n",
    "\n",
    "__Randomness:__ \n",
    "\n",
    "* __`temperature`:__ Controls the randomness of the generated sequences. This parameter is between zero and one. When set closer to zero, the model tends to select higher probability words. When set further away from zero, the model may select lower-probability words. Temperature of zero gives the same output for the same input at every run.\n",
    "* __`topP`:__ Top P defines a cut-off based on the sum of probabilities of the potential choices. With this cut-off, the model only selects from the most probable words whose probabilities sum up to the topP value. \n",
    "\n",
    "__Length:__ \n",
    "\n",
    "* __`maxTokens`:__ Controls the maximum number of tokens in the generated response.\n",
    "* __`stopSequences`:__ A sequence of characters to stop the model from generating its output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166ec99",
   "metadata": {},
   "source": [
    "Let's define a function that will build our prompt as well as pass some model parameters to Bedrock. Once the call parameters are ready, we can use the __invoke_model()__ function to send the data and collect the response from the model. Then, we return the response at the end.\n",
    "\n",
    "**Note:** Different models may have different model parameters. Please refer to the [documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/service_code_examples_bedrock-runtime.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6381affa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send_prompt(prompt_data, temperature=0.0, top_p=1.0, max_token_count=1000):\n",
    "\n",
    "    body = json.dumps(\n",
    "        {\n",
    "            \"inputText\": prompt_data,\n",
    "            \"textGenerationConfig\": {\n",
    "                \"temperature\": temperature,\n",
    "                \"topP\": top_p,\n",
    "                \"maxTokenCount\": max_token_count,\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "\n",
    "    modelId = \"amazon.titan-text-premier-v1:0\"\n",
    "    accept = \"application/json\"\n",
    "    contentType = \"application/json\"\n",
    "\n",
    "    response = bedrock_inference.invoke_model(\n",
    "        body=body, modelId=modelId, accept=accept, contentType=contentType\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "\n",
    "    return response_body[\"results\"][0][\"outputText\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0a8d86",
   "metadata": {},
   "source": [
    "Let's construct a simple API call and run it. \n",
    "\n",
    "As a sample text input, we use the following: __\"Can you name a few real-life applications of natural language processing?\"__. \n",
    "\n",
    "For inference parameters, we set the __`temperature`__ to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c77b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "prompt_data = (\n",
    "    \"\"\"Can you name a few real-life applications of natural language processing?\"\"\"\n",
    ")\n",
    "\n",
    "display(Markdown(prompt_data))\n",
    "display(Markdown(send_prompt(prompt_data, temperature=0.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b2072",
   "metadata": {},
   "source": [
    "This provides a list of real-life NLP applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76c800d",
   "metadata": {},
   "source": [
    "If we want to generate slightly different looking outputs, we can increase the __`temperature`__ value. Let's also set the __`maxTokens`__ parameter this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd14f46",
   "metadata": {},
   "source": [
    "Changing the temperature parameter will create a slightly different looking list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2288738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = (\n",
    "    \"\"\"Can you name a few real-life applications of natural language processing?\"\"\"\n",
    ")\n",
    "\n",
    "display(Markdown(prompt_data))\n",
    "display(Markdown(send_prompt(prompt_data, temperature=0.5, max_token_count=450)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be01d5",
   "metadata": {},
   "source": [
    "In addition to the __`temperature`__ and __`maxTokens`__ parameters, we can also add in the __`topP`__ parameter to set a cut-off for the sum of the probabilities of the potential words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7099dfd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_data = (\n",
    "    \"\"\"Can you name a few real-life applications of natural language processing?\"\"\"\n",
    ")\n",
    "\n",
    "display(Markdown(prompt_data))\n",
    "display(\n",
    "    Markdown(send_prompt(prompt_data, max_token_count=450, temperature=0.5, top_p=0.7))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce30a83-0bd1-4b2b-adb3-cf669b630055",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### <a name=\"4\">Quiz Questions</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Well done on completing the lab! Now, it's time for a brief knowledge assessment.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"../mlu_utils/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Answer the following questions to test your understanding of using LLMs for inference.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a585fb7-7f8f-40b7-9668-b6eb97be845a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>var Quiz=function(){\"use strict\";var M=document.createElement(\"style\");M.textContent=`.quiz-wrapper.svelte-fk6ar3{padding:1rem}.footer.svelte-fk6ar3{display:flex;align-items:center}h2.svelte-fk6ar3{font-size:1.5rem;margin-bottom:2rem;color:#232f3e}p.svelte-fk6ar3{font-size:16px}.options.svelte-fk6ar3{display:grid;grid-template-columns:repeat(2,50%);grid-template-rows:repeat(2,1fr);width:100%;margin:auto;justify-content:center}.mlu-quizquestion-option-button.svelte-fk6ar3{padding:1rem;margin:.5rem}.submit-button.svelte-fk6ar3{padding:1rem;margin:.5rem;width:90px;color:#fff;background-color:coral}.active.svelte-fk6ar3{background-color:#232f3e;color:#fff}.correct-answer.svelte-fk6ar3{background-color:green}.incorrect-answer.svelte-fk6ar3{background-color:red;text-decoration:line-through}.available.svelte-fk6ar3{pointer-events:none;opacity:.6}\n",
       "`,document.head.appendChild(M);function I(){}function P(e){return e()}function W(){return Object.create(null)}function O(e){e.forEach(P)}function X(e){return typeof e==\"function\"}function Z(e,t){return e!=e?t==t:e!==t||e&&typeof e==\"object\"||typeof e==\"function\"}function x(e){return Object.keys(e).length===0}function m(e,t){e.appendChild(t)}function j(e,t,n){e.insertBefore(t,n||null)}function z(e){e.parentNode&&e.parentNode.removeChild(e)}function ee(e,t){for(let n=0;n<e.length;n+=1)e[n]&&e[n].d(t)}function k(e){return document.createElement(e)}function A(e){return document.createTextNode(e)}function S(){return A(\" \")}function te(){return A(\"\")}function Y(e,t,n,r){return e.addEventListener(t,n,r),()=>e.removeEventListener(t,n,r)}function v(e,t,n){n==null?e.removeAttribute(t):e.getAttribute(t)!==n&&e.setAttribute(t,n)}function ne(e){return Array.from(e.childNodes)}function T(e,t){t=\"\"+t,e.wholeText!==t&&(e.data=t)}function p(e,t,n){e.classList[n?\"add\":\"remove\"](t)}let L;function N(e){L=e}const $=[],D=[],Q=[],F=[],re=Promise.resolve();let B=!1;function oe(){B||(B=!0,re.then(H))}function R(e){Q.push(e)}const G=new Set;let E=0;function H(){if(E!==0)return;const e=L;do{try{for(;E<$.length;){const t=$[E];E++,N(t),le(t.$$)}}catch(t){throw $.length=0,E=0,t}for(N(null),$.length=0,E=0;D.length;)D.pop()();for(let t=0;t<Q.length;t+=1){const n=Q[t];G.has(n)||(G.add(n),n())}Q.length=0}while($.length);for(;F.length;)F.pop()();B=!1,G.clear(),N(e)}function le(e){if(e.fragment!==null){e.update(),O(e.before_update);const t=e.dirty;e.dirty=[-1],e.fragment&&e.fragment.p(e.ctx,t),e.after_update.forEach(R)}}const ie=new Set;function se(e,t){e&&e.i&&(ie.delete(e),e.i(t))}function ce(e,t,n,r){const{fragment:l,after_update:i}=e.$$;l&&l.m(t,n),r||R(()=>{const s=e.$$.on_mount.map(P).filter(X);e.$$.on_destroy?e.$$.on_destroy.push(...s):O(s),e.$$.on_mount=[]}),i.forEach(R)}function fe(e,t){const n=e.$$;n.fragment!==null&&(O(n.on_destroy),n.fragment&&n.fragment.d(t),n.on_destroy=n.fragment=null,n.ctx=[])}function ue(e,t){e.$$.dirty[0]===-1&&($.push(e),oe(),e.$$.dirty.fill(0)),e.$$.dirty[t/31|0]|=1<<t%31}function ae(e,t,n,r,l,i,s,g=[-1]){const c=L;N(e);const o=e.$$={fragment:null,ctx:[],props:i,update:I,not_equal:l,bound:W(),on_mount:[],on_destroy:[],on_disconnect:[],before_update:[],after_update:[],context:new Map(t.context||(c?c.$$.context:[])),callbacks:W(),dirty:g,skip_bound:!1,root:t.target||c.$$.root};s&&s(o.root);let b=!1;if(o.ctx=n?n(e,t.props||{},(a,q,...y)=>{const h=y.length?y[0]:q;return o.ctx&&l(o.ctx[a],o.ctx[a]=h)&&(!o.skip_bound&&o.bound[a]&&o.bound[a](h),b&&ue(e,a)),q}):[],o.update(),b=!0,O(o.before_update),o.fragment=r?r(o.ctx):!1,t.target){if(t.hydrate){const a=ne(t.target);o.fragment&&o.fragment.l(a),a.forEach(z)}else o.fragment&&o.fragment.c();t.intro&&se(e.$$.fragment),ce(e,t.target,t.anchor,t.customElement),H()}N(c)}class de{$destroy(){fe(this,1),this.$destroy=I}$on(t,n){if(!X(n))return I;const r=this.$$.callbacks[t]||(this.$$.callbacks[t]=[]);return r.push(n),()=>{const l=r.indexOf(n);l!==-1&&r.splice(l,1)}}$set(t){this.$$set&&!x(t)&&(this.$$.skip_bound=!0,this.$$set(t),this.$$.skip_bound=!1)}}const be=\"\";function J(e,t,n){const r=e.slice();return r[11]=t[n],r[13]=n,r}function K(e){let t,n=e[11]+\"\",r,l,i,s;function g(){return e[9](e[13])}return{c(){t=k(\"button\"),r=A(n),l=S(),v(t,\"class\",\"mlu-quizquestion-option-button button svelte-fk6ar3\"),p(t,\"active\",e[5]===e[13]),p(t,\"correct-answer\",e[1]&&e[5]===e[13]&&e[2]==e[0].correctIndex),p(t,\"incorrect-answer\",e[1]&&e[5]===e[13]&&e[2]!=e[0].correctIndex),p(t,\"available\",e[4]&&e[1])},m(c,o){j(c,t,o),m(t,r),m(t,l),i||(s=Y(t,\"click\",g),i=!0)},p(c,o){e=c,o&1&&n!==(n=e[11]+\"\")&&T(r,n),o&32&&p(t,\"active\",e[5]===e[13]),o&39&&p(t,\"correct-answer\",e[1]&&e[5]===e[13]&&e[2]==e[0].correctIndex),o&39&&p(t,\"incorrect-answer\",e[1]&&e[5]===e[13]&&e[2]!=e[0].correctIndex),o&18&&p(t,\"available\",e[4]&&e[1])},d(c){c&&z(t),i=!1,s()}}}function U(e){let t;function n(i,s){return i[3]==!0?he:_e}let r=n(e),l=r(e);return{c(){l.c(),t=te()},m(i,s){l.m(i,s),j(i,t,s)},p(i,s){r!==(r=n(i))&&(l.d(1),l=r(i),l&&(l.c(),l.m(t.parentNode,t)))},d(i){l.d(i),i&&z(t)}}}function _e(e){let t;return{c(){t=k(\"p\"),t.textContent=\"This is not the correct answer. Try again!\",v(t,\"class\",\"svelte-fk6ar3\")},m(n,r){j(n,t,r)},d(n){n&&z(t)}}}function he(e){let t;return{c(){t=k(\"p\"),t.textContent=\"Good! You got the correct answer.\",v(t,\"class\",\"svelte-fk6ar3\")},m(n,r){j(n,t,r)},d(n){n&&z(t)}}}function me(e){let t,n,r=e[0].question+\"\",l,i,s,g,c,o,b=e[1]?\"Retry\":\"Submit\",a,q,y,h,C=e[0].options,d=[];for(let f=0;f<C.length;f+=1)d[f]=K(J(e,C,f));let _=e[1]&&U(e);return{c(){t=k(\"div\"),n=k(\"h2\"),l=A(r),i=S(),s=k(\"div\");for(let f=0;f<d.length;f+=1)d[f].c();g=S(),c=k(\"div\"),o=k(\"button\"),a=A(b),q=S(),_&&_.c(),v(n,\"class\",\"svelte-fk6ar3\"),v(s,\"class\",\"options svelte-fk6ar3\"),v(o,\"class\",\"submit-button svelte-fk6ar3\"),p(o,\"available\",!e[4]),v(c,\"class\",\"footer svelte-fk6ar3\"),v(t,\"class\",\"quiz-wrapper svelte-fk6ar3\")},m(f,w){j(f,t,w),m(t,n),m(n,l),m(t,i),m(t,s);for(let u=0;u<d.length;u+=1)d[u].m(s,null);m(t,g),m(t,c),m(c,o),m(o,a),m(c,q),_&&_.m(c,null),y||(h=Y(o,\"click\",e[10]),y=!0)},p(f,[w]){if(w&1&&r!==(r=f[0].question+\"\")&&T(l,r),w&311){C=f[0].options;let u;for(u=0;u<C.length;u+=1){const V=J(f,C,u);d[u]?d[u].p(V,w):(d[u]=K(V),d[u].c(),d[u].m(s,null))}for(;u<d.length;u+=1)d[u].d(1);d.length=C.length}w&2&&b!==(b=f[1]?\"Retry\":\"Submit\")&&T(a,b),w&16&&p(o,\"available\",!f[4]),f[1]?_?_.p(f,w):(_=U(f),_.c(),_.m(c,null)):_&&(_.d(1),_=null)},i:I,o:I,d(f){f&&z(t),ee(d,f),_&&_.d(),y=!1,h()}}}function pe(e,t,n){let{question:r={question:\"Who didn't attend this meeting?\",options:[\"Xin\",\"Anand\",\"Brent\"],correctIndex:2}}=t,l=!1,i=-1,s=\"no\",g=!1,c;function o(){n(4,g=!1),n(1,l=!1),n(2,i=-1),n(5,c=-1),n(3,s=\"no\")}function b(){n(1,l=!0),n(3,s=i==r.correctIndex)}function a(h){n(4,g=!0),n(2,i=h),n(5,c=h)}const q=h=>a(h),y=()=>l?o():b();return e.$$set=h=>{\"question\"in h&&n(0,r=h.question)},[r,l,i,s,g,c,o,b,a,q,y]}class ge extends de{constructor(t){super(),ae(this,t,pe,me,Z,{question:0})}}return ge}();\n",
       "</script>\n",
       "        \n",
       "        <div id=\"Quiz-204b5ac1\"></div>\n",
       "        <script>\n",
       "        (() => {\n",
       "            var data = {\n",
       "\"question\": {\n",
       "\"question\": \"Which model parameter controls randomness or diversity in generated sequences?\",\n",
       "\"options\": [\n",
       "\"temperature\",\n",
       "\"topP\",\n",
       "\"MaxTokens\",\n",
       "\"stopSequences\"\n",
       "],\n",
       "\"correctIndex\": 0\n",
       "}\n",
       "};\n",
       "            window.Quiz_data = data;\n",
       "            var Quiz_inst = new Quiz({\n",
       "                \"target\": document.getElementById(\"Quiz-204b5ac1\"),\n",
       "                \"props\": data\n",
       "            });\n",
       "        })();\n",
       "        </script>\n",
       "        \n",
       "        "
      ],
      "text/plain": [
       "<mlu_utils.quiz_questions.Quiz at 0x7ff69851ee00>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from mlu_utils.quiz_questions import *\n",
    "\n",
    "lab2a_question1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe8f9a",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "\n",
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"../mlu_utils/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

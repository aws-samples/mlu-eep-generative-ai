{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0d1368",
   "metadata": {},
   "source": [
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"../mlu_utils/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>\n",
    "\n",
    "\n",
    "# <a name=\"0\">Machine Learning University</a>\n",
    "## <a name=\"0\">Lab4b: Tree of Thoughts (ToT)</a>\n",
    "\n",
    "This notebook introduces the Tree of Thoughts prompting strategy, that guides generative models for text to generate, evaluate, expand on, and decide among multiple solutions. The process is similar to how humans solve problems by evaluating various potential solutions before deciding on the most promising one.\n",
    "\n",
    "1. <a href=\"#1\">Import libraries</a>\n",
    "2. <a href=\"#2\">Set up Bedrock for inference</a>\n",
    "3. <a href=\"#3\">Tree of Thoughts (ToT)</a>\n",
    "    - <a href=\"#31\">Creative writing task</a>\n",
    "    - <a href=\"#32\">Prompts and custom class to run the creative writing task</a>\n",
    "    - <a href=\"#33\">Baseline 1: Standard prompting</a>\n",
    "    - <a href=\"#34\">Baseline 2: Chain-of-thought prompting</a>\n",
    "    - <a href=\"#35\">Tree-of-thought prompting</a>\n",
    "4. <a href=\"#4\">Evaluation of results</a>\n",
    "5. <a href=\"#5\">Quizzes</a>\n",
    "    \n",
    "Please work top to bottom of this notebook and don't skip sections as this could lead to error messages due to missing code.\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "You will be presented with coding activities to check your knowledge and understanding throughout the notebook whenever you see the MLU robot:\n",
    "\n",
    "<img style=\"display: block; margin-left: auto; margin-right: auto;\" src=\"../mlu_utils/activity.png\" alt=\"Activity\" width=\"125\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3020b5",
   "metadata": {},
   "source": [
    "### <a name=\"1\">1. Import libraries</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's start by installing all required packages as specified in the `requirements.txt` file and importing several libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dc09b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b81748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import boto3\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import textwrap\n",
    "from collections import Counter\n",
    "\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from igraph import Graph\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mlu_utils.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21e229-b076-4472-bb42-549e79738b3c",
   "metadata": {},
   "source": [
    "### <a name=\"2\">2. Set up Bedrock for inference</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "To get started, set up Bedrock and instantiate an active `bedrock-runtime` to query LLMs. \n",
    "\n",
    "Let us define a custom function to run inference with Bedrock-hosted models. The code below leverages [LangChain's Bedrock integration](https://python.langchain.com/docs/integrations/llms/bedrock) and allows to use Bedrock-hosted models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e39bc-3f4a-4f12-b753-1d8ace2a0d6e",
   "metadata": {},
   "source": [
    "Next, use Bedrock for inference to test everything works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ef78f-9ee8-42f9-bad0-53451e940b10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example model prompting\n",
    "\n",
    "MODEL = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "TEMP = 0.8\n",
    "SAMPLES = 2\n",
    "\n",
    "greeting_prompt = \"<s>[INST]How are you doing?[/INST]\"\n",
    "Markdown(generate_outputs(greeting_prompt, MODEL, TEMP, max_tokens=128, n=5)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6709fa4e",
   "metadata": {},
   "source": [
    "### <a name=\"3\">3. Tree of Thoughts (ToT)</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "The Tree of Thoughts (ToT) framework was proposed in 2023 in the following two papers:\n",
    "* [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601), Yao et al. (2023)\n",
    "* [Large Language Model Guided Tree-of-Thought](https://arxiv.org/abs/2305.08291), Long (2023)\n",
    "\n",
    "ToT is motivated by the difficulties that LLMs prompted with simple techniques encounter when trying to solve complex tasks that require exploration or strategic lookahead. ToT generalizes over Chain of Thought (CoT) and encourages exploration over \"thoughts\" that serve as intermediate steps for general problem solving with LLMs. CoT prompting samples thoughts sequentially, but ToT prompting follows a tree-branching technique. With the ToT technique, the LLM can consider multiple paths instead of one sequential path. ToT is an especially effective method for tasks that involve important initial decisions, strategies for the future, and exploration of multiple solutions. \n",
    "\n",
    "Below you will see how to use a Bedrock-hosted LLM to solve a task using the ToT technique. You will compare the results with those given by standard and CoT prompts.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"attachment:ToT_Diagram.png\" width=\"600\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8ccbb",
   "metadata": {},
   "source": [
    "#### <a name=\"31\">3.1. Creative writing task</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will build on the creating writing exercise shown in Yao et al. which prompts the LLM to write a coherent passage of a few paragraphs constrained to some fixed sentences. For baselines they use zero-shot standard and CoT prompts. While the former prompts the LLM to directly generate a coherent passage given input constraints, the latter prompts to first make a brief plan then write the passage, i.e. **the plan serves as the intermediate thought step.** \n",
    "\n",
    "For ToT we will build a tree with depth 2 (and only 1 intermediate thought step). The LLM is prompted to generate `k` plans and vote for the most promising one, then similarly generate `k` passages based on the best plan to finally vote for the best one. A simple zero-shot vote prompt is used to sample `j` votes at both steps. The LLM will be tasked to write a customer press release."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af51d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amazon Local Gift Cards PR\n",
    "title = \"Amazon introduces Local Gift Cards\"\n",
    "start = \"Gift cards are consistently among the most sought after gifts year after year.\"\n",
    "mid = '''\"Happy Anniversary\"'''\n",
    "end = '''\"And we'll continue to work towards our vision of allowing our customers to find and purchase any gift card they need.\"'''\n",
    "\n",
    "input_data = (title, start, mid, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427af00",
   "metadata": {},
   "source": [
    "#### <a name=\"32\">3.2. Prompts and custom class to run the creative writing task</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "For this exercise we will be using several prompts, that we keep in a separate file `prompts.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b155d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load all prompts from a separate file prompts.py\n",
    "\n",
    "from mlu_utils.prompts import (\n",
    "    standard_prompt_template,\n",
    "    cot_prompt_template,\n",
    "    plan_prompt_template,\n",
    "    vote_plan_prompt_template,\n",
    "    vote_passage_prompt_template,\n",
    "    score_coherency_prompt_template,\n",
    "    compare_prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01cc11",
   "metadata": {},
   "source": [
    "To facilitate running experiments, we have created a custom class that implements ToT and allows to generate outputs with standard, CoT, and ToT prompting techniques. \n",
    "\n",
    "The code below implements all needed methods. **You don't need to understand every detail**. Later in the notebook we will use the class step by step to run the ToT approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINES = [\"standard\", \"cot\"]\n",
    "VOTES = [\"vote_plan\", \"vote_passage\"]\n",
    "PASSAGE_STR = \"Passage:\"\n",
    "PLAN_STR = \"Plan:\"\n",
    "\n",
    "TEMPLATES = {\n",
    "    \"standard\": standard_prompt_template,\n",
    "    \"cot\": cot_prompt_template,\n",
    "    \"vote_plan\": vote_plan_prompt_template,\n",
    "    \"vote_passage\": vote_passage_prompt_template,\n",
    "}\n",
    "\n",
    "\n",
    "class TextTaskToT:\n",
    "    \"\"\"\n",
    "    Class to run ToT for creative writing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_data, model, temperature):\n",
    "        self.title, self.start, self.mid, self.end = input_data\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.votes_outputs = {}\n",
    "        self.best_choice = {}\n",
    "\n",
    "    def wrap_prompt(self, method, extra_args=None):\n",
    "        \"\"\"\n",
    "        Returns different prompts used in this task\n",
    "        Supported methods: standard, cot, plan, vote_plan, vote_passage\n",
    "        \"\"\"\n",
    "        if method in BASELINES:\n",
    "            prompt = PromptTemplate.from_template(TEMPLATES[method]).format(\n",
    "                title=self.title, start=self.start, mid=self.mid, end=self.end\n",
    "            )\n",
    "\n",
    "        if method == \"plan\":\n",
    "            prompt = PromptTemplate.from_template(plan_prompt_template).format(\n",
    "                plan=extra_args,\n",
    "                title=self.title,\n",
    "                start=self.start,\n",
    "                mid=self.mid,\n",
    "                end=self.end,\n",
    "            )\n",
    "\n",
    "        if method in VOTES:\n",
    "            enumerated_choices = \"\"\n",
    "            for i, choice in enumerate(extra_args, 1):\n",
    "                enumerated_choices += f\"Choice {i}: \\n{choice}\\n\\n\"\n",
    "\n",
    "            prompt = PromptTemplate.from_template(TEMPLATES[method]).format(\n",
    "                enumerated_choices=enumerated_choices.strip()\n",
    "            )\n",
    "        return prompt\n",
    "\n",
    "    def generate_tot_thoughts(self, n_branches):\n",
    "        \"\"\"\n",
    "        Generate n_branches thoughts to start the tree of thoughts\n",
    "        \"\"\"\n",
    "        # CoT prompt to generate the branches for ToT\n",
    "        prompt = self.wrap_prompt(\"cot\")\n",
    "        # Stop generating in \"Passage:\" since we want the plans only\n",
    "        thoughts = generate_outputs(\n",
    "            prompt,\n",
    "            self.model,\n",
    "            self.temperature,\n",
    "            stop_sequences=[PASSAGE_STR],\n",
    "            n=n_branches,\n",
    "        )\n",
    "        # Ensure that the thoughts are clean of \"Passage:\" and \"Plan:\" markers\n",
    "        thoughts = [self.sanitize_passage(t, PLAN_STR, position=1) for t in thoughts]\n",
    "        thoughts = [self.sanitize_passage(t, PASSAGE_STR, position=0) for t in thoughts]\n",
    "        return thoughts\n",
    "\n",
    "    def vote_tot_alternatives(self, method, choices, n_rounds_vote):\n",
    "        \"\"\"\n",
    "        Choose best alternative by voting (majority vote)\n",
    "        \"\"\"\n",
    "        # method is \"vote_plan\" to choose among thoughts or \"vote_passage\" to choose among passages\n",
    "        vote_prompt = self.wrap_prompt(method, choices)\n",
    "        # Store votes outputs for later inspection\n",
    "        self.votes_outputs[method] = generate_outputs(\n",
    "            vote_prompt, self.model, self.temperature, n=n_rounds_vote\n",
    "        )\n",
    "        votes = [self.extract_vote(vote_out) for vote_out in self.votes_outputs[method]]\n",
    "        votes_ctr = Counter(votes)\n",
    "        print(votes_ctr)\n",
    "        majority_vote = votes_ctr.most_common(1)[0][0]\n",
    "        if majority_vote is None:\n",
    "            majority_vote = 0\n",
    "        # Majority vote determines best choice\n",
    "        print(f\"choices :: {choices}\")\n",
    "        self.best_choice[method] = choices[majority_vote]\n",
    "        return self.best_choice[method], majority_vote\n",
    "\n",
    "    def make_tot_passage(self, n_branches, n_rounds_vote):\n",
    "        \"\"\"\n",
    "        Run ToT method end-to-end\n",
    "        \"\"\"\n",
    "        print(f\"--- Generating {n_branches} plans...\")\n",
    "        self.thoughts = self.generate_tot_thoughts(n_branches)\n",
    "        print(f\"--- Voting for most promising plan in {n_rounds_vote} rounds...\")\n",
    "        best_plan, best_plan_index = self.vote_tot_alternatives(\n",
    "            \"vote_plan\", self.thoughts, n_rounds_vote\n",
    "        )\n",
    "        print(f\"--- Generating {n_branches} passages from most promising plan...\")\n",
    "        prompt_plan = self.wrap_prompt(\"plan\", best_plan)\n",
    "        passages = generate_outputs(\n",
    "            prompt_plan, self.model, self.temperature, n=n_branches\n",
    "        )\n",
    "        print(f\"--- Voting for most coherent passage in {n_rounds_vote} rounds...\")\n",
    "        best_passage, best_package_index = self.vote_tot_alternatives(\n",
    "            \"vote_passage\", passages, n_rounds_vote\n",
    "        )\n",
    "        best_passage = self.sanitize_passage(best_passage, PASSAGE_STR, position=1)\n",
    "        print(\"Done.\\n\")\n",
    "        return best_passage\n",
    "\n",
    "    def make_passages(self, method, n=1, n_branches=5, n_rounds_vote=3):\n",
    "        \"\"\"\n",
    "        Wrapper to generate passages with the three methods\n",
    "        \"\"\"\n",
    "        if method in BASELINES:\n",
    "            prompt = ttt.wrap_prompt(method)\n",
    "            passages = generate_outputs(prompt, self.model, self.temperature, n=n)\n",
    "            passages = [\n",
    "                self.sanitize_passage(p, PASSAGE_STR, position=1) for p in passages\n",
    "            ]\n",
    "\n",
    "        if method == \"tot\":\n",
    "            passages = []\n",
    "            for i in range(n):\n",
    "                passages.append(self.make_tot_passage(n_branches, n_rounds_vote))\n",
    "\n",
    "        return passages\n",
    "\n",
    "    @staticmethod\n",
    "    def sanitize_passage(passage, string_to_clean, position):\n",
    "        \"\"\"\n",
    "        Util function to remove unwanted markers in text\n",
    "        \"\"\"\n",
    "        # position is 0 if we keep text before, 1 if we keep text after\n",
    "        if string_to_clean in passage:\n",
    "            passage = passage.split(string_to_clean)[position].strip()\n",
    "        return passage\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_vote(vote_output: str) -> int:\n",
    "        \"\"\"\n",
    "        Parser to extract the best choice when the LLM is voting\n",
    "        \"\"\"\n",
    "        pattern = r\".*best choice is (\\d+).*\"\n",
    "        match = re.match(pattern, vote_output, re.DOTALL)\n",
    "        vote = None\n",
    "        if match:\n",
    "            vote = int(match.groups()[0]) - 1\n",
    "        return vote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d51678f",
   "metadata": {},
   "source": [
    "#### <a name=\"33\">3.3. Baseline 1: Standard prompting</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will use [Mistral model](https://aws.amazon.com/bedrock/mistral/) hosted in Bedrock to generate passages. Let's start by generating a passage with the standard prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47abedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"mistral.mistral-7b-instruct-v0:2\"\n",
    "TEMP = 0.75\n",
    "\n",
    "ttt = TextTaskToT(input_data, MODEL, TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138012b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(ttt.wrap_prompt(\"standard\")))\n",
    "display(Markdown(\"---\"))\n",
    "standard_passage = ttt.make_passages(\"standard\")[0]\n",
    "display(Markdown(standard_passage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371f3d1",
   "metadata": {},
   "source": [
    "#### <a name=\"34\">3.4. Baseline 2: Chain-of-thought prompting</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "\n",
    "Let's continue with a CoT prompt that asks the LLM to first produce a plan (\"thought\") before writing the passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57358cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(ttt.wrap_prompt(\"cot\")))\n",
    "display(Markdown(\"---\"))\n",
    "cot_passage = ttt.make_passages(\"cot\")[0]\n",
    "display(Markdown(cot_passage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda65ce5",
   "metadata": {},
   "source": [
    "#### <a name=\"35\">3.5. Tree-of-thought prompting</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Finally let's use Tree of Thoughts to improve on this passage generation. The system will undergo the following steps: \n",
    "- produce `k` plans as \"thoughts\" \n",
    "- vote for the most promising one\n",
    "- generate `k` passages using the most promising thought\n",
    "- vote for the most coherent passage\n",
    "\n",
    "Let's show a step by step example using `k=3` branches for the number of thoughts. \n",
    "\n",
    "1. First we generate the 3 thoughts. \n",
    "2. We then ask the model to vote for the most promising using a simple vote prompt (see `vote_plan_prompt_template` in `prompts.py`).\n",
    "3. Next we generate multiple passages from the best plan. \n",
    "4. Finally the system is again tasked with voting for the \"best\" passage, understood as the most coherent. \n",
    "\n",
    "The following cell takes 1-2 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6553f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 3 thoughts\n",
    "thoughts = ttt.generate_tot_thoughts(n_branches=3)\n",
    "\n",
    "# Vote for the best plan\n",
    "best_plan, best_plan_index = ttt.vote_tot_alternatives(\"vote_plan\", thoughts, n_rounds_vote=3)\n",
    "\n",
    "# Generate passages with the best plan\n",
    "prompt_plan = ttt.wrap_prompt(\"plan\", best_plan)\n",
    "candidate_passages = generate_outputs(prompt_plan, ttt.model, ttt.temperature, n=3)\n",
    "\n",
    "# Vote for the best package\n",
    "best_passage, best_passage_index = ttt.vote_tot_alternatives(\n",
    "    \"vote_passage\", candidate_passages, n_rounds_vote=3\n",
    ")\n",
    "Markdown(best_passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a41b4-823a-4e5f-861a-645dc2ab7ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidate_passages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74802dec",
   "metadata": {},
   "source": [
    "#### Visualize ToT logic\n",
    "\n",
    "Let's plot a tree to help us visualize the steps followed by the ToT method. We will use `igraph` and `plotly` for that. \n",
    "\n",
    "**You don't need to understand every detail of the plotting code.** Here's some [plotly documentation](https://plotly.com/python/tree-plots/) if you're interested in using the library yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(matrix):\n",
    "    \"\"\" Function to flatten a list of lists \"\"\"\n",
    "    return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "def wrap_text(text, width=40):\n",
    "    \"\"\" Function to wrap text into a fixed size box for display \"\"\"\n",
    "    text = \"\\n\".join(\n",
    "        [\n",
    "            \"\\n\".join(\n",
    "                textwrap.wrap(\n",
    "                    line, width, break_long_words=False, replace_whitespace=False\n",
    "                )\n",
    "            )\n",
    "            for line in text.splitlines()\n",
    "            if line.strip() != \"\"\n",
    "        ]\n",
    "    )\n",
    "    return text.replace(\"\\n\", \"<br>\")\n",
    "\n",
    "\n",
    "# Hard-coded to this example with k=3 thoughts\n",
    "nr_vertices = 7\n",
    "best_plan_index_tree = 1 + best_plan_index\n",
    "best_passage_index_tree = 4 + best_passage_index\n",
    "edges = [(0, 1), (0, 2), (0, 3), (best_plan_index_tree, 4), (best_plan_index_tree, 5), (best_plan_index_tree, 6)]\n",
    "G = Graph(edges=edges)\n",
    "lay = G.layout('rt', root=[0])\n",
    "\n",
    "# Coordinates of nodes and edges\n",
    "Xn, Yn = zip(*lay.coords)\n",
    "Yn = [-y for y in Yn]\n",
    "Xe = flatten([(lay.coords[edge[0]][0], lay.coords[edge[1]][0], None) for edge in edges])\n",
    "Ye = flatten([(-lay.coords[edge[0]][1], -lay.coords[edge[1]][1], None) for edge in edges])\n",
    "\n",
    "\n",
    "# Put the proper labels into the nodes of the tree\n",
    "labels = [\n",
    "    \"Tree-of-Thoughts root\",\n",
    "    wrap_text(thoughts[0]),\n",
    "    wrap_text(thoughts[1]),\n",
    "    wrap_text(thoughts[2]),\n",
    "    wrap_text(candidate_passages[0].split(\"\\nPlan\")[1], width=80),\n",
    "    wrap_text(candidate_passages[1].split(\"\\nPlan\")[1], width=80),\n",
    "    wrap_text(candidate_passages[2].split(\"\\nPlan\")[1], width=80)\n",
    "]\n",
    "\n",
    "# Assemble everything and plot\n",
    "lo = go.Layout(\n",
    "    autosize=False, width=1200, height=400,\n",
    "    xaxis=go.layout.XAxis(linecolor=\"white\", linewidth=1, mirror=True, showgrid=False, showticklabels=False),\n",
    "    yaxis=go.layout.YAxis(linecolor=\"white\", linewidth=1, mirror=True, showgrid=False, showticklabels=False),\n",
    "    margin=go.layout.Margin(l=0, r=0, b=0, t=50, pad=0),\n",
    ")\n",
    "\n",
    "fig = go.Figure(layout=lo)\n",
    "fig.update_layout(plot_bgcolor=\"white\", showlegend=False, title=\"Tree-of-thoughts with k=3 and depth=2\")\n",
    "# Add edges\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=Xe, y=Ye, mode=\"lines\", line=dict(color=\"lightgray\", width=2), hoverinfo=\"none\",)\n",
    ")\n",
    "# Add nodes\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=Xn, y=Yn, mode=\"markers\", name=\"\",\n",
    "        marker=dict(\n",
    "            symbol=\"circle-dot\",\n",
    "            size=18,\n",
    "            color=\"royalblue\",\n",
    "            line=dict(color=\"royalblue\", width=1),\n",
    "        ),\n",
    "        text=labels, hoverinfo=\"text\", opacity=0.8,\n",
    "    )\n",
    ")\n",
    "# Update with best alternatives\n",
    "bestXn = [Xn[best_plan_index_tree], Xn[best_passage_index_tree]]\n",
    "bestYn = [Yn[best_plan_index_tree], Yn[best_passage_index_tree]]\n",
    "bestlabels = [labels[best_plan_index_tree], labels[best_passage_index_tree]]\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=bestXn, y=bestYn, mode=\"markers\", name=\"\",\n",
    "        marker=dict(\n",
    "            symbol=\"circle-dot\",\n",
    "            size=18,\n",
    "            color=\"seagreen\",\n",
    "            line=dict(color=\"seagreen\", width=1),\n",
    "        ),\n",
    "        text=bestlabels, hoverinfo=\"text\", opacity=0.8,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d6229-176c-4652-92d2-44fac15ef928",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto; padding-left: 50px; padding-right: 50px\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"../mlu_utils/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">You can inspect the reasons why the system voted for one or the other thoughts and passages. </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Try looking into <code>ttt.votes_outputs</code>. You will find keys <code>vote_plan</code> and <code>vote_passage</code> containing the analysis that yielded the best options. Majority vote decided the output. Do you agree with the decision made by the system?</p>\n",
    "    <br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ce2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "\n",
    "############## END OF CODE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1fe793",
   "metadata": {},
   "source": [
    "#### Run ToT end-to-end\n",
    "\n",
    "We can now generate a ToT passage using the wrapper function. We will use `k=5` for the number of thoughts and `j=3` for the number of times we ask the LLM to vote in both levels of the tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2977cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_passage = ttt.make_passages(\"tot\", n_branches=5, n_rounds_vote=3)[0]\n",
    "display(Markdown(tot_passage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc7245",
   "metadata": {},
   "source": [
    "### <a name=\"4\">4. Evaluation of results</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "A key aspect of LLM usage is the evaluation of its results. In this example we will use a second LLM, Mistral 7b model, to assess the coherency of pairs of passages. Following Yao et al. we implement two methods: \n",
    "\n",
    "- ask the evaluator LLM to provide coherency score from 1 to 10\n",
    "- ask the evaluator LLM to compare two passages and assess which one is more coherent. \n",
    "\n",
    "To faciliate experimentation, we provide a class for evaluation below that implements both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa1cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateToT:\n",
    "    \"\"\"\n",
    "    Class to evaluate ToT against baselines\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, temperature):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def compare_passages(self, passage1, passage2, n_rounds=1):\n",
    "        \"\"\"\n",
    "        Ask LLM to compare passages and decide which one is more coherent\n",
    "        \"\"\"\n",
    "        compare_prompt = PromptTemplate.from_template(compare_prompt_template).format(\n",
    "            passage1=passage1, passage2=passage2\n",
    "        )\n",
    "        comparisons = generate_outputs(\n",
    "            compare_prompt, self.model, self.temperature, n=n_rounds\n",
    "        )\n",
    "        return comparisons\n",
    "\n",
    "    def score_passages(self, passage, n_rounds=1):\n",
    "        \"\"\"\n",
    "        Ask LLM to score the coherency of a passage from 1 to 10\n",
    "        \"\"\"\n",
    "        score_coherency_prompt = PromptTemplate.from_template(\n",
    "            score_coherency_prompt_template\n",
    "        ).format(passage=passage)\n",
    "        scoring = generate_outputs(\n",
    "            score_coherency_prompt, self.model, self.temperature, n=n_rounds\n",
    "        )\n",
    "        return scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d865bf7-5d2f-4313-b1ad-562b819bf23e",
   "metadata": {},
   "source": [
    "Below are the passages generated by the three methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283ef431",
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = {\"standard\": standard_passage, \"cot\": cot_passage, \"tot\": tot_passage}\n",
    "\n",
    "for method, passage in passages.items():\n",
    "    display(Markdown(f\"### Passage with {method} prompting:\"))\n",
    "    display(Markdown(passage))\n",
    "    display(Markdown(\"---\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab578c",
   "metadata": {},
   "source": [
    "### Evaluating coherence via LLM scoring.\n",
    "\n",
    "Let's instantiate the evaluator with the Mistral 7b model. For the first evaluation task, we prompt the model to score the coherency of each passage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70879d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_EVAL = \"amazon.titan-text-premier-v1:0\"\n",
    "TEMP_EVAL = 0.75\n",
    "\n",
    "ett = EvaluateToT(MODEL_EVAL, TEMP_EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2aceb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in passages.keys():\n",
    "    display(Markdown(f\"### {method}\"))\n",
    "    display(Markdown((ett.score_passages(passages[method])[0])))\n",
    "    display(Markdown((\"---\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c43daa",
   "metadata": {},
   "source": [
    "### Evaluating coherence via pair comparison.\n",
    "\n",
    "For the second evaluation task, we construct pairs of passages and ask the evaluator LLM to choose the more coherent of the two. To double check consistency of the LLM as evaluator, we test the coherency of a passage with itself (it should be equally coherent) and we also flip the order of the evaluated passages (theoretically the LLM should not care about the order, but in practice we observe a certain tendency to prefer the first appearing passage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0471e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    (\"standard\", \"standard\"),\n",
    "    (\"tot\", \"tot\"),\n",
    "    (\"standard\", \"cot\"),\n",
    "    (\"cot\", \"standard\"),\n",
    "    (\"standard\", \"tot\"),\n",
    "    (\"tot\", \"standard\"),\n",
    "    (\"cot\", \"tot\"),\n",
    "    (\"tot\", \"cot\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a764c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pairs:\n",
    "    passage1, passage2 = pair\n",
    "    display(Markdown((f\"### 1. {passage1}\\t2. {passage2}\")))\n",
    "    display(Markdown((ett.compare_passages(passages[passage1], passages[passage2])[0])))\n",
    "    display(Markdown((\"---\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31f3fd",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We have implemented the Tree-of-Thoughts prompting approach as introduced in Yao et al. (2023) and have applied it to the same type of creative writing task showcased there (the other 2 are solving a mini crossword and the mathematical reasoning challenge Game of 24). For creative writing, the generation of plans that serve as thoughts allow the system to consider several paths and choose the most promising one. \n",
    "\n",
    "Notice that due to the variability in the generated outputs, and the fact that LLMs as evaluators tend to prefer the choice presented as first option, it is not always possible to observe a clear superiority of the ToT approach with respect to the baselines. The ToT paper presents results from multiple runs using OpenAI models and are able to demonstrate that ToT produces improved results than standard and CoT prompting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953e3fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto; padding-left: 50px; padding-right: 50px\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"../mlu_utils/activity.png\" alt=\"Activity\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\"><b>Well done on completing the lab. Now it's time for you to get creative.</b></p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Try changing the input data to prompt the model to produce passages based on different data. You can also modify the model id and temperature settings to explore the variability of the results.</p>\n",
    "    <br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a40cbd-dc6c-4c88-8e8d-4fa17c78d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## CODE HERE ####################\n",
    "\n",
    "\n",
    "\n",
    "############## END OF CODE ##################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65859b4-3494-42c3-8a5c-5564765e8878",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### <a name=\"5\">Quiz Questions</a>\n",
    "\n",
    "Well done on completing the lab! Now, it's time for a brief knowledge assessment.\n",
    "\n",
    "<div style=\"border: 4px solid coral; text-align: center; margin: auto;\">\n",
    "    <h2><i>Try it Yourself!</i></h2>\n",
    "    <br>\n",
    "    <p style=\"text-align:center;margin:auto;\"><img src=\"../mlu_utils/challenge.png\" alt=\"Challenge\" width=\"100\" /> </p>\n",
    "    <p style=\" text-align: center; margin: auto;\">Answer the following questions to test your understanding of topics.</p>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db35629e-69c7-4968-abc4-743c33f985b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from mlu_utils.quiz_questions import *\n",
    "\n",
    "lab4b_question1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee696c1",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "\n",
    "<p style=\"padding: 10px; border: 1px solid black;\">\n",
    "<img src=\"../mlu_utils/MLU-NEW-logo.png\" alt=\"drawing\" width=\"400\"/> <br/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
